{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How to run this notebook (command-line)?**\n",
    "1. Install the `ReinventCommunity` environment:\n",
    "`conda env create -f environment.yml`\n",
    "2. Activate the environment:\n",
    "`conda activate ReinventCommunity`\n",
    "3. Execute `jupyter`:\n",
    "`jupyter notebook`\n",
    "4. Copy the link to a browser\n",
    "\n",
    "\n",
    "# `REINVENT 3.2`: Model building demo\n",
    "For many applications of `REINVENT`, we already have some prior knowledge on a project that we would like to incorporate. One example of how this can be achieved are *predictive models* trained on a collection of compounds, for example `QSAR` models that relate the structure of a compound to an activity / potency endpoint. In this demo, we will explain how to build `scikit-learn` models that are compatible with `REINVENT`. Note, that our model will have the ending `.pkl`, as we need to save the model's parameters in a serialized fashion (\"[pickled](https://docs.python.org/3/library/pickle.html)\").\n",
    "\n",
    "Our input dataset will be `DRD2` (see also the \"complete use-case notebook\"), for which we have compiled two `CSV` files with the compounds' `SMILES` in one column and a read-out value in another (see below). We will train a `Random Forest` regressor here, but you can choose any backend algorithm that provides a `scikit-learn` interface. As of now, `REINVENT` supports 4 different kinds of molecular fingerprints and we will need to calculate them before we can train our model.\n",
    "\n",
    "The following imports are required to execute the code. Please update the output directory path if you want to store the temporary files and the final model in another place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import sklearn.ensemble\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, MACCSkeys, PandasTools\n",
    "from rdkit.Avalon import pyAvalonTools\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set plotting parameters\n",
    "large = 22; med = 16; small = 12\n",
    "params = {'axes.titlesize': large,\n",
    "          'legend.fontsize': med,\n",
    "          'figure.figsize': (16, 10),\n",
    "          'axes.labelsize': med,\n",
    "          'axes.titlesize': med,\n",
    "          'xtick.labelsize': med,\n",
    "          'ytick.labelsize': med,\n",
    "          'figure.titlesize': large}\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is /home/springnuance/reinvent-hitl/Reinvent-Community-Binh/notebooks\n"
     ]
    }
   ],
   "source": [
    "print(\"The current working directory is\", os.getcwd())\n",
    "\n",
    "# --------- change these path variables as required\n",
    "output_dir = os.path.expanduser(\"./REINVENT_model_building_demo\")\n",
    "\n",
    "# --------- do not change\n",
    "# get the notebook's root path\n",
    "try: ipynb_path\n",
    "except NameError: ipynb_path = os.getcwd()\n",
    "\n",
    "# if required, generate a folder to store the results\n",
    "try:\n",
    "    os.mkdir(output_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# copy data sets to temporary folder for inspection\n",
    "copyfile(os.path.join(ipynb_path, \"data/drd2.train.csv\"),\n",
    "         os.path.join(output_dir, \"drd2.train.csv\"))\n",
    "copyfile(os.path.join(ipynb_path, \"data/drd2.test.csv\"),\n",
    "         os.path.join(output_dir, \"drd2.test.csv\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Before proceeding, let us inspect the datasets. We have split them into a training and test sets, each of which has `canonical` (the `SMILES`) and `activity` (either `1` or `0`, corresponding to `active` and `inactive` respectively) columns. This means, we have a (binary) classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# obs in train:  275768\n",
      "# obs in test:  68944\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonical</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COc1ccc(NC(=O)CC2C(=O)N(c3ccc(Cl)cc3)C(=S)N2CC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=C(CSc1oc(-c2ccccc2)nc1S(=O)(=O)c1ccc(Br)cc1)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cn1c(=S)n(CCC(=O)O)c2ccccc21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clc1ccc(NN=Cc2ccncc2)cc1Cl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=C(C=NO)NCCCNCc1ccccc1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           canonical  activity\n",
       "0  COc1ccc(NC(=O)CC2C(=O)N(c3ccc(Cl)cc3)C(=S)N2CC...         0\n",
       "1  O=C(CSc1oc(-c2ccccc2)nc1S(=O)(=O)c1ccc(Br)cc1)...         0\n",
       "2                       Cn1c(=S)n(CCC(=O)O)c2ccccc21         0\n",
       "3                         Clc1ccc(NN=Cc2ccncc2)cc1Cl         0\n",
       "4                            O=C(C=NO)NCCCNCc1ccccc1         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(output_dir, \"drd2.train.csv\"))\n",
    "test = pd.read_csv(os.path.join(output_dir, \"drd2.test.csv\"))\n",
    "\n",
    "\n",
    "print(\"# obs in train: \", train.shape[0])\n",
    "print(\"# obs in test: \", test.shape[0])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in activity\n",
      "We can see that activity = 1 is extremely rare compared to activity = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    272320\n",
       "1      3448\n",
       "Name: activity, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Unique values in activity\")\n",
    "print(\"We can see that activity = 1 is extremely rare compared to activity = 0\")\n",
    "train[\"activity\"].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptors\n",
    "As mentioned above, we need to calculate descriptors (fingerprints) which are understood by `REINVENT`. Below is some code to facilitate this for one example configuration (`ECFP6` with counts), but feel free to adapt it if you feel your model could benefit from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_mols(query_smiles):\n",
    "\n",
    "    mols = [Chem.MolFromSmiles(smile) for smile in query_smiles]\n",
    "    valid = [0 if mol is None else 1 for mol in mols]\n",
    "    valid_idxs = [idx for idx, boolean in enumerate(valid) if boolean == 1]\n",
    "    valid_mols = [mols[idx] for idx in valid_idxs]\n",
    "    return valid_mols, valid_idxs\n",
    "\n",
    "def smiles_to_mols_batches(query_smiles, batch_size=10000):\n",
    "    batched_mols = []\n",
    "    batched_idxs = []\n",
    "    # print(len(query_smiles))\n",
    "    for i in range(0, len(query_smiles), batch_size):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing batch {i} to {i+batch_size}\")\n",
    "        batch = query_smiles[i:i+batch_size]\n",
    "        mols, idxs = smiles_to_mols(batch)\n",
    "        batched_mols.extend(mols)\n",
    "        # Adjust index according to the batch\n",
    "        adjusted_idxs = [i + idx for idx in idxs]\n",
    "        batched_idxs.extend(adjusted_idxs)\n",
    "    return batched_mols, batched_idxs\n",
    "\n",
    "def getMorganFingerprints(mols, radius, useFeatures, useCounts):\n",
    "    fps = [AllChem.GetMorganFingerprint(mol, radius, useCounts=useCounts, useFeatures=useFeatures) for mol in mols]\n",
    "    return fps\n",
    "\n",
    "class Descriptors:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self._data = data\n",
    "\n",
    "    def ECFP(self, radius, nBits):\n",
    "        fingerprints = []\n",
    "        mols, idx = smiles_to_mols(self._data)\n",
    "        \n",
    "        fp_bits = [AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits) for mol in mols]\n",
    "        \n",
    "        for fp in fp_bits:\n",
    "            fp_np = np.zeros((1, nBits), dtype=np.int32)\n",
    "            DataStructs.ConvertToNumpyArray(fp, fp_np)\n",
    "            fingerprints.append(fp_np)\n",
    "        return fingerprints, idx\n",
    "\n",
    "    def ECFP_counts(self, radius, useFeatures, useCounts=True):\n",
    "        \n",
    "        #mols, valid_idx = smiles_to_mols(self._data)\n",
    "        mols, valid_idx = smiles_to_mols_batches(self._data)\n",
    "        print(\"Finish converting smiles to mols\")\n",
    "        fps = [AllChem.GetMorganFingerprint(mol, radius, useCounts=useCounts, useFeatures=useFeatures) for mol in mols]\n",
    "        print(\"Finish getting Morgan fingerprints\")\n",
    "        size = 2048\n",
    "        nfp = np.zeros((len(fps), size), np.int32)\n",
    "        print(\"The shape of nfp is\", nfp.shape)\n",
    "        for i, fp in enumerate(fps):\n",
    "            for idx, v in fp.GetNonzeroElements().items():\n",
    "                nidx = idx % size\n",
    "                nfp[i, nidx] += int(v)\n",
    "        return nfp, valid_idx\n",
    "\n",
    "    def Avalon(self, nBits):\n",
    "        mols, valid_idx = smiles_to_mols(self._data)\n",
    "        fingerprints = []\n",
    "        fps = [pyAvalonTools.GetAvalonFP(mol, nBits=nBits) for mol in mols]\n",
    "        for fp in fps:\n",
    "            fp_np = np.zeros((1, nBits), dtype=np.int32)\n",
    "            DataStructs.ConvertToNumpyArray(fp, fp_np)\n",
    "            fingerprints.append(fp_np)\n",
    "        return fingerprints, valid_idx\n",
    "\n",
    "    def MACCS_keys(self):\n",
    "        mols, valid_idx = smiles_to_mols(self._data)\n",
    "        fingerprints = []\n",
    "        fps = [MACCSkeys.GenMACCSKeys(mol) for mol in mols]\n",
    "        for fp in fps:\n",
    "            fp_np = np.zeros((1, ), dtype=np.int32)\n",
    "            DataStructs.ConvertToNumpyArray(fp, fp_np)\n",
    "            fingerprints.append(fp_np)\n",
    "        return fingerprints, valid_idx\n",
    "    \n",
    "def get_ECFP6_counts(inp):\n",
    "    if not isinstance(inp, list):\n",
    "        inp = list(inp)\n",
    "    desc = Descriptors(inp)\n",
    "    fps, _ = desc.ECFP_counts(radius=3, useFeatures=True, useCounts=True)\n",
    "    return fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting valid mols and valid idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_mols, train_valid_idxs = smiles_to_mols_batches(train[\"canonical\"], batch_size=10000)\n",
    "np.save(os.path.join(output_dir, \"mols_and_idxs/train_valid_mols.npy\"), train_valid_mols)\n",
    "np.save(os.path.join(output_dir, \"mols_and_idxs/train_valid_idxs.npy\"), train_valid_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_valid_mols, test_valid_idxs = smiles_to_mols_batches(test[\"canonical\"], batch_size=10000)\n",
    "np.save(os.path.join(output_dir, \"mols_and_idxs/test_valid_mols.npy\"), test_valid_mols)\n",
    "np.save(os.path.join(output_dir, \"mols_and_idxs/test_valid_idxs.npy\"), test_valid_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Morgan Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_mols = np.load(os.path.join(output_dir, \"mols_and_idxs/train_valid_mols.npy\"), allow_pickle=True).tolist()\n",
    "train_fps = getMorganFingerprints(train_valid_mols, radius=3, useFeatures=True, useCounts=True)\n",
    "np.save(os.path.join(output_dir, \"Morgan_fingerprints/train_fps.npy\"), train_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_valid_mols = np.load(os.path.join(output_dir, \"mols_and_idxs/test_valid_mols.npy\"), allow_pickle=True).tolist()\n",
    "test_fps = getMorganFingerprints(test_valid_mols, radius=3, useFeatures=True, useCounts=True)\n",
    "np.save(os.path.join(output_dir, \"Morgan_fingerprints/test_fps.npy\"), test_fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Nonzero Morgan Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/28\n",
      "  Processing fingerprint 0 in batch 1\n",
      "Batch 1 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_1.npy\n",
      "Processing batch 2/28\n",
      "  Processing fingerprint 0 in batch 2\n",
      "Batch 2 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_2.npy\n",
      "Processing batch 3/28\n",
      "  Processing fingerprint 0 in batch 3\n",
      "Batch 3 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_3.npy\n",
      "Processing batch 4/28\n",
      "  Processing fingerprint 0 in batch 4\n",
      "Batch 4 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_4.npy\n",
      "Processing batch 5/28\n",
      "  Processing fingerprint 0 in batch 5\n",
      "Batch 5 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_5.npy\n",
      "Processing batch 6/28\n",
      "  Processing fingerprint 0 in batch 6\n",
      "Batch 6 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_6.npy\n",
      "Processing batch 7/28\n",
      "  Processing fingerprint 0 in batch 7\n",
      "Batch 7 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_7.npy\n",
      "Processing batch 8/28\n",
      "  Processing fingerprint 0 in batch 8\n",
      "Batch 8 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_8.npy\n",
      "Processing batch 9/28\n",
      "  Processing fingerprint 0 in batch 9\n",
      "Batch 9 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_9.npy\n",
      "Processing batch 10/28\n",
      "  Processing fingerprint 0 in batch 10\n",
      "Batch 10 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_10.npy\n",
      "Processing batch 11/28\n",
      "  Processing fingerprint 0 in batch 11\n",
      "Batch 11 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_11.npy\n",
      "Processing batch 12/28\n",
      "  Processing fingerprint 0 in batch 12\n",
      "Batch 12 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_12.npy\n",
      "Processing batch 13/28\n",
      "  Processing fingerprint 0 in batch 13\n",
      "Batch 13 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_13.npy\n",
      "Processing batch 14/28\n",
      "  Processing fingerprint 0 in batch 14\n",
      "Batch 14 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_14.npy\n",
      "Processing batch 15/28\n",
      "  Processing fingerprint 0 in batch 15\n",
      "Batch 15 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_15.npy\n",
      "Processing batch 16/28\n",
      "  Processing fingerprint 0 in batch 16\n",
      "Batch 16 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_16.npy\n",
      "Processing batch 17/28\n",
      "  Processing fingerprint 0 in batch 17\n",
      "Batch 17 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_17.npy\n",
      "Processing batch 18/28\n",
      "  Processing fingerprint 0 in batch 18\n",
      "Batch 18 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_18.npy\n",
      "Processing batch 19/28\n",
      "  Processing fingerprint 0 in batch 19\n",
      "Batch 19 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_19.npy\n",
      "Processing batch 20/28\n",
      "  Processing fingerprint 0 in batch 20\n",
      "Batch 20 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_20.npy\n",
      "Processing batch 21/28\n",
      "  Processing fingerprint 0 in batch 21\n",
      "Batch 21 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_21.npy\n",
      "Processing batch 22/28\n",
      "  Processing fingerprint 0 in batch 22\n",
      "Batch 22 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_22.npy\n",
      "Processing batch 23/28\n",
      "  Processing fingerprint 0 in batch 23\n",
      "Batch 23 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_23.npy\n",
      "Processing batch 24/28\n",
      "  Processing fingerprint 0 in batch 24\n",
      "Batch 24 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_24.npy\n",
      "Processing batch 25/28\n",
      "  Processing fingerprint 0 in batch 25\n",
      "Batch 25 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_25.npy\n",
      "Processing batch 26/28\n",
      "  Processing fingerprint 0 in batch 26\n",
      "Batch 26 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_26.npy\n",
      "Processing batch 27/28\n",
      "  Processing fingerprint 0 in batch 27\n",
      "Batch 27 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_27.npy\n",
      "Processing batch 28/28\n",
      "  Processing fingerprint 0 in batch 28\n",
      "Batch 28 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/train_nfp_28.npy\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "size = 2048\n",
    "\n",
    "train_fps = np.load(os.path.join(output_dir, \"Morgan_fingerprints/train_fps.npy\"), allow_pickle=True).tolist()\n",
    "\n",
    "num_batches = len(train_fps) // batch_size + (1 if len(train_fps) % batch_size > 0 else 0)\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    print(f\"Processing batch {batch_num + 1}/{num_batches}\")\n",
    "    \n",
    "    # Calculate start and end index for current batch\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = min((batch_num + 1) * batch_size, len(train_fps))\n",
    "    \n",
    "    # Initialize the nfp array for this batch\n",
    "    nfp_batch = np.zeros((end_idx - start_idx, size), np.int16)\n",
    "    \n",
    "    for i in range(start_idx, end_idx):\n",
    "        # Update the progress every 10000 fingerprints within the batch if needed\n",
    "        if (i - start_idx) % 10000 == 0:\n",
    "            print(f\"  Processing fingerprint {i - start_idx} in batch {batch_num + 1}\")\n",
    "        \n",
    "        fp = train_fps[i]\n",
    "        for idx, v in fp.GetNonzeroElements().items():\n",
    "            nidx = idx % size\n",
    "            nfp_batch[i - start_idx, nidx] += int(v)\n",
    "    \n",
    "    # Save the current batch\n",
    "    batch_filename = os.path.join(output_dir, f\"nonzero_Morgan_fingerprints/train_nfp_{batch_num + 1}.npy\")\n",
    "    np.save(batch_filename, nfp_batch)\n",
    "    print(f\"Batch {batch_num + 1} saved as {batch_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/7\n",
      "  Processing fingerprint 0 in batch 1\n",
      "Batch 1 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/test_nfp_1.npy\n",
      "Processing batch 2/7\n",
      "  Processing fingerprint 0 in batch 2\n",
      "Batch 2 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/test_nfp_2.npy\n",
      "Processing batch 3/7\n",
      "  Processing fingerprint 0 in batch 3\n",
      "Batch 3 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/test_nfp_3.npy\n",
      "Processing batch 4/7\n",
      "  Processing fingerprint 0 in batch 4\n",
      "Batch 4 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/test_nfp_4.npy\n",
      "Processing batch 5/7\n",
      "  Processing fingerprint 0 in batch 5\n",
      "Batch 5 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/test_nfp_5.npy\n",
      "Processing batch 6/7\n",
      "  Processing fingerprint 0 in batch 6\n",
      "Batch 6 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/test_nfp_6.npy\n",
      "Processing batch 7/7\n",
      "  Processing fingerprint 0 in batch 7\n",
      "Batch 7 saved as ./REINVENT_model_building_demo/nonzero_Morgan_fingerprints/test_nfp_7.npy\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "size = 2048\n",
    "\n",
    "test_fps = np.load(os.path.join(output_dir, \"Morgan_fingerprints/test_fps.npy\"), allow_pickle=True).tolist()\n",
    "\n",
    "num_batches = len(test_fps) // batch_size + (1 if len(test_fps) % batch_size > 0 else 0)\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    print(f\"Processing batch {batch_num + 1}/{num_batches}\")\n",
    "    \n",
    "    # Calculate start and end index for current batch\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = min((batch_num + 1) * batch_size, len(test_fps))\n",
    "    \n",
    "    # Initialize the nfp array for this batch\n",
    "    nfp_batch = np.zeros((end_idx - start_idx, size), np.int16)\n",
    "    \n",
    "    for i in range(start_idx, end_idx):\n",
    "        # Update the progress every 10000 fingerprints within the batch if needed\n",
    "        if (i - start_idx) % 10000 == 0:\n",
    "            print(f\"  Processing fingerprint {i - start_idx} in batch {batch_num + 1}\")\n",
    "        \n",
    "        fp = test_fps[i]\n",
    "        for idx, v in fp.GetNonzeroElements().items():\n",
    "            nidx = idx % size\n",
    "            nfp_batch[i - start_idx, nidx] += int(v)\n",
    "    \n",
    "    # Save the current batch\n",
    "    batch_filename = os.path.join(output_dir, f\"nonzero_Morgan_fingerprints/test_nfp_{batch_num + 1}.npy\")\n",
    "    np.save(batch_filename, nfp_batch)\n",
    "    print(f\"Batch {batch_num + 1} saved as {batch_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "To train your model (i.e. fit the model's parameters to the training data), execute the following cell. Note, that `REINVENT` will access the `proba` property of the model to get a probability rather than a predicted label. If you want to optimize the hyper-parameters of you model, we suggest you use a cross-validation approach (we aim to publish our in-house method based on [optuna](https://optuna.org) soon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(train_fps).shape)\n",
    "print(train[\"activity\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding batch 1/28\n",
      "Adding batch 2/28\n",
      "Adding batch 3/28\n",
      "Adding batch 4/28\n",
      "Adding batch 5/28\n",
      "Adding batch 6/28\n",
      "Adding batch 7/28\n",
      "Adding batch 8/28\n",
      "Adding batch 9/28\n",
      "Adding batch 10/28\n",
      "Adding batch 11/28\n",
      "Adding batch 12/28\n",
      "Adding batch 13/28\n",
      "Adding batch 14/28\n",
      "Adding batch 15/28\n",
      "Adding batch 16/28\n",
      "Adding batch 17/28\n",
      "Adding batch 18/28\n",
      "Adding batch 19/28\n",
      "Adding batch 20/28\n",
      "Adding batch 21/28\n",
      "Adding batch 22/28\n",
      "Adding batch 23/28\n",
      "Adding batch 24/28\n",
      "Adding batch 25/28\n",
      "Adding batch 26/28\n",
      "Adding batch 27/28\n",
      "Adding batch 28/28\n",
      "(275768, 2048)\n"
     ]
    }
   ],
   "source": [
    "train_nfp = []\n",
    "\n",
    "batch_size = 10000\n",
    "\n",
    "train_fps = np.load(os.path.join(output_dir, \"Morgan_fingerprints/train_fps.npy\"), allow_pickle=True).tolist()\n",
    "\n",
    "num_batches = len(train_fps) // batch_size + (1 if len(train_fps) % batch_size > 0 else 0)\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    print(f\"Adding batch {batch_num + 1}/{num_batches}\")\n",
    "    train_nfp_batch_i = np.load(os.path.join(output_dir, f\"nonzero_Morgan_fingerprints/train_nfp_{batch_num + 1}.npy\"))\n",
    "    train_nfp.append(train_nfp_batch_i)\n",
    "\n",
    "train_nfp = np.concatenate(train_nfp, axis=0)\n",
    "print(train_nfp.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding batch 1/7\n",
      "Adding batch 2/7\n",
      "Adding batch 3/7\n",
      "Adding batch 4/7\n",
      "Adding batch 5/7\n",
      "Adding batch 6/7\n",
      "Adding batch 7/7\n",
      "(68944, 2048)\n"
     ]
    }
   ],
   "source": [
    "test_nfp = []\n",
    "\n",
    "batch_size = 10000\n",
    "\n",
    "test_fps = np.load(os.path.join(output_dir, \"Morgan_fingerprints/test_fps.npy\"), allow_pickle=True).tolist()\n",
    "\n",
    "num_batches = len(test_fps) // batch_size + (1 if len(test_fps) % batch_size > 0 else 0)\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    print(f\"Adding batch {batch_num + 1}/{num_batches}\")\n",
    "    test_nfp_batch_i = np.load(os.path.join(output_dir, f\"nonzero_Morgan_fingerprints/test_nfp_{batch_num + 1}.npy\"))\n",
    "    test_nfp.append(test_nfp_batch_i)\n",
    "\n",
    "test_nfp = np.concatenate(test_nfp, axis=0)\n",
    "print(test_nfp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a \"Random Forest Classifier\" (choice of hyper-parameters somewhat arbitrary)\n",
    "RFclassifier = sklearn.ensemble.RandomForestClassifier(max_depth=20,\n",
    "                                                       max_features=\"auto\",\n",
    "                                                       n_estimators=100,\n",
    "                                                       class_weight=\"balanced\")\n",
    "\n",
    "# fit to training data\n",
    "RFclassifier.fit(train_nfp, train[\"activity\"])\n",
    "y_pred = RFclassifier.predict(X=train_nfp)\n",
    "train_score = roc_auc_score(y_true=train[\"activity\"], y_score=y_pred)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the classifier\n",
    "with open(os.path.join(output_dir, \"trained_classifiers/DRD2_only_train_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(RFclassifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance on test set\n",
    "To get a better idea on how well (or meager) our model performs, we can evaluate it on the test (hold-out) set. These observations have not been used for fitting the parameters and are thus a good proxy for how the model would fare for new compounds generated, e.g. by `REINVENT`. Note, that we do not strive to give a comprehensive tutorial how to build good, well generalizing models in this demonstration and there are quite a few important considerations to contemplate before actually using a model in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9644378348788165\n"
     ]
    }
   ],
   "source": [
    "RFclassifier = pickle.load(open(os.path.join(output_dir, \"trained_classifiers/DRD2_only_train_model.pkl\"), \"rb\"))\n",
    "\n",
    "y_pred_test = RFclassifier.predict(X=test_nfp)\n",
    "test_score = roc_auc_score(y_true=test[\"activity\"], y_score=y_pred_test)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out final model\n",
    "Finally, we will write-out the model. Usually, we will build it using all the observations available. While this maximizes the amount of knowledge available to the model, it is noteworthy that this comes at the trade-off of not being able to independently assess the model's performance anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFclassifier_final = sklearn.ensemble.RandomForestClassifier(max_depth=20,\n",
    "                                                             max_features=\"auto\",\n",
    "                                                             n_estimators=100,\n",
    "                                                             class_weight=\"balanced\")\n",
    "\n",
    "# fit to all data points\n",
    "complete_nfp = np.concatenate((train_nfp, test_nfp), axis=0)\n",
    "complete_y = pd.concat((train[\"activity\"], test[\"activity\"]))\n",
    "RFclassifier_final.fit(complete_nfp, complete_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "with open(os.path.join(output_dir, \"trained_classifiers/DRD2_final_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(RFclassifier_final, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "node array from the pickle has an incompatible dtype:\n- expected: [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]\n- got     : {'names':['left_child','right_child','feature','threshold','impurity','n_node_samples','weighted_n_node_samples','missing_go_to_left'], 'formats':['<i8','<i8','<i8','<f8','<f8','<i8','<f8','u1'], 'offsets':[0,8,16,24,32,40,48,56], 'itemsize':64}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b74f45048653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRFclassifier_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"trained_classifiers/DRD2_final_model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcomplete_nfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_nfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_nfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcomplete_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"activity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"activity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFclassifier_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomplete_nfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/tree/_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree.__setstate__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/tree/_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree._check_node_ndarray\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: node array from the pickle has an incompatible dtype:\n- expected: [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]\n- got     : {'names':['left_child','right_child','feature','threshold','impurity','n_node_samples','weighted_n_node_samples','missing_go_to_left'], 'formats':['<i8','<i8','<i8','<f8','<f8','<i8','<f8','u1'], 'offsets':[0,8,16,24,32,40,48,56], 'itemsize':64}"
     ]
    }
   ],
   "source": [
    "RFclassifier_final = pickle.load(open(os.path.join(output_dir, \"trained_classifiers/DRD2_final_model.pkl\"), \"rb\"))\n",
    "\n",
    "complete_nfp = np.concatenate((train_nfp, test_nfp), axis=0)\n",
    "complete_y = pd.concat((train[\"activity\"], test[\"activity\"]))\n",
    "y_pred_final = RFclassifier_final.predict(X=complete_nfp)\n",
    "train_score = roc_auc_score(y_true=complete_y, y_score=y_pred_final)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration into `REINVENT`\n",
    "In order to use your new model as a component in the scoring function of `REINVENT`, you need to include a block with the appropriate parameter settings (see below). Note, that the descriptor definition needs to match.\n",
    "\n",
    "```\n",
    "{\n",
    "  \"component_type\": \"predictive_property\",\n",
    "  \"name\": \"DRD2_pred_activity\",\n",
    "  \"weight\": 1,\n",
    "  \"specific_parameters\": {\n",
    "    \"model_path\": \"/path/to/model/folder/DRD2_final_model.pkl\",\n",
    "    \"scikit\": \"classification\",\n",
    "    \"transformation: {\n",
    "        \"transformation_type\": \"no_transformation\"\n",
    "    },\n",
    "    \"descriptor_type\": \"ecfp_counts\",\n",
    "    \"size\": 2048,\n",
    "    \"radius\": 3,\n",
    "    \"use_counts\": True,\n",
    "    \"use_features\": True\n",
    "  }\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
