{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import Crippen\n",
    "from rdkit import DataStructs\n",
    "from numpy.random import default_rng\n",
    "import torch\n",
    "from ast import literal_eval\n",
    "from torch import nn, optim\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from tdc import Oracle\n",
    "import subprocess\n",
    "\n",
    "from rdkit.Chem import RDConfig\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "# now you can import sascore!\n",
    "import sascorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/springnuance/reinvent-hitl/Base-Code-Binh\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_Bradley_Terry_model.bradley_terry import BradleyTerryModel\n",
    "from training_Rank_ListNet_model.rank_listnet import RankListNetModel\n",
    "from training_Score_Regression_model.score_regression import ScoreRegressionModel\n",
    "from scripts.helper import load_drd2_dataset, write_REINVENT_config, change_config_json, \\\n",
    "                    read_scaffold_result, load_feedback_model, smiles_human_score, \\\n",
    "                    compute_fingerprints, retrain_feedback_model,\\\n",
    "                    create_drd2_dataset, combine_drd2_dataset, save_drd2_dataset\n",
    "                        \n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, matthews_corrcoef\n",
    "\n",
    "def predict_proba_from_model_fps(feedback_type, feedback_model, features):\n",
    "\n",
    "    # This is not computationally extensive yet\n",
    "    # choose float 32\n",
    "    \n",
    "    features = torch.tensor(features, dtype=torch.float32)\n",
    "    pred_label_proba = feedback_model.predict_proba(features).cpu().detach().numpy()\n",
    "    return pred_label_proba\n",
    "\n",
    "\n",
    "def check_create(path):\n",
    "    \"\"\"\n",
    "    Check if the directory exists, if not, create it.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "def evaluate_results(\n",
    "        output_dir, benchmark, # Choose whether to evaluate the results of the benchmark\n",
    "        feedback_type, # scoring, comparing, ranking\n",
    "        initial_feedback_model_path, # path to the initial feedback model\n",
    "        final_testing_dataset_path, # path to the final training dataset\n",
    "        model_pretrained_name, # Name of the pretrained model before REINVENT_round_1\n",
    "        num_rounds, # number of rounds, corresponding to R in the paper\n",
    "        num_iters, # number of iterations of showing molecules to the human for feedback at each round, corresponding to T in the paper\n",
    "):\n",
    "    \"\"\"\n",
    "    This function returns \n",
    "    - metrics: dictionary of metrics for each REINVENT round\n",
    "    - drd2_proba: dictionary of DRD2 probabilities for each REINVENT round\n",
    "    - novelty_score: dictionary of novelty scores for each REINVENT round\n",
    "    - sa_score: dictionary of synthetic accessibility scores for each REINVENT round\n",
    "    - qed_score: dictionary of QED scores for each REINVENT round\n",
    "    - logP: dictionary of logP scores for each REINVENT round\n",
    "    - mol_weight: dictionary of molecular weight scores for each REINVENT round\n",
    "    - h_donors: dictionary of hydrogen bond donors for each REINVENT round\n",
    "    - h_acceptors: dictionary of hydrogen bond acceptors for each REINVENT round\n",
    "    - tpsa: dictionary of topological polar surface area for each REINVENT round\n",
    "    - rotatable_bonds: dictionary of rotatable bonds for each REINVENT round\n",
    "    - num_rings: dictionary of number of rings for each REINVENT round\n",
    "    \"\"\"\n",
    "    \n",
    "    final_testing_dataset = pd.read_csv(final_testing_dataset_path)\n",
    "\n",
    "    print(\"Loading final testing dataset successfully\")\n",
    "        \n",
    "    smiles_test = final_testing_dataset['smiles'].to_numpy()\n",
    "    label_test = final_testing_dataset['label'].to_numpy()\n",
    "    \n",
    "    # for score regression, dataset_outputs have columns: smiles, features, label_proba, label_binary\n",
    "    # for bradley-terry, dataset_outputs have columns: smiles_1, smiles_2, features_1, features_2, \n",
    "                                                      # label_1_proba, label_2_proba, label_1_binary, label_2_binary\n",
    "                                                      # compare_proba, compare_binary\n",
    "    # for rank listnet, dataset_outputs have columns: smiles_1/2/3, features_1/2/3, label_1/2/3_proba, label_1/2/3_binary\n",
    "                                                     # label_1/2/3_softmax, label_1/2/3_rank\n",
    "    \n",
    "    ##########################\n",
    "    # SAVING THE SMILES DATA #\n",
    "    ##########################\n",
    "    if benchmark[\"smiles_list\"] == True:\n",
    "        smiles_list_dict = {}\n",
    "        for REINVENT_round in range(1, num_rounds + 2):\n",
    "            reinvent_round_name = f\"REINVENT_round_{REINVENT_round}\"\n",
    "            \n",
    "            scaffold_memory_path = f\"{output_dir}/{reinvent_round_name}/results/scaffold_memory.csv\"\n",
    "            \n",
    "            if os.path.exists(scaffold_memory_path):\n",
    "                scaffold_df = pd.read_csv(scaffold_memory_path)\n",
    "                smiles_list_round = scaffold_df['SMILES'].to_numpy()\n",
    "                smiles_list_dict[reinvent_round_name] = smiles_list_round\n",
    "            else:\n",
    "                print(f\"Scaffold memory file {scaffold_memory_path} does not exist.\")\n",
    "\n",
    "\n",
    "    ##############\n",
    "    # ML METRICS #\n",
    "    ##############\n",
    "    \n",
    "    features_list = [compute_fingerprints(smiles) for smiles in smiles_test]\n",
    "    features = np.array(features_list, dtype=np.float32)\n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    if benchmark[\"metrics\"] == True:\n",
    "        metrics_model = {\n",
    "            'thresholds': [],\n",
    "            'TP': [], 'TN': [], 'FP': [], 'FN': [],\n",
    "            'accuracy': [], 'precision': [], 'recall': [], 'F1': [], 'MCC': [],\n",
    "        }\n",
    "        \n",
    "        initial_feedback_model = load_feedback_model(feedback_type=feedback_type, \n",
    "                                                    feedback_model_path=initial_feedback_model_path)\n",
    "        predicted_scores = predict_proba_from_model_fps(feedback_type, initial_feedback_model, features)\n",
    "\n",
    "        # Compute metrics over a range of thresholds\n",
    "        thresholds = np.linspace(0, 1, 101)\n",
    "        for threshold in thresholds:\n",
    "            predicted_labels = (predicted_scores > threshold).astype(int)\n",
    "            TP = np.sum((predicted_labels == 1) & (label_test == 1))\n",
    "            TN = np.sum((predicted_labels == 0) & (label_test == 0))\n",
    "            FP = np.sum((predicted_labels == 1) & (label_test == 0))\n",
    "            FN = np.sum((predicted_labels == 0) & (label_test == 1))\n",
    "\n",
    "            accuracy = (TP + TN) / len(label_test)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(label_test, predicted_labels, average='binary')\n",
    "            mcc = matthews_corrcoef(label_test, predicted_labels)\n",
    "\n",
    "            metrics_model['thresholds'].append(threshold)\n",
    "            metrics_model['TP'].append(TP)\n",
    "            metrics_model['TN'].append(TN)\n",
    "            metrics_model['FP'].append(FP)\n",
    "            metrics_model['FN'].append(FN)\n",
    "            metrics_model['accuracy'].append(accuracy)\n",
    "            metrics_model['precision'].append(precision)\n",
    "            metrics_model['recall'].append(recall)\n",
    "            metrics_model['F1'].append(f1)\n",
    "            metrics_model['MCC'].append(mcc)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(label_test, predicted_scores, pos_label=1)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        metrics_model['fpr'] = fpr\n",
    "        metrics_model['tpr'] = tpr\n",
    "        metrics_model['roc_auc'] = roc_auc\n",
    "        metrics[\"REINVENT_round_1\"] = metrics_model\n",
    "        \n",
    "        ########################\n",
    "        # HITL MODEL BENCHMARK #\n",
    "        ########################\n",
    "\n",
    "        for REINVENT_round in range(2, num_rounds + 2):\n",
    "            reinvent_round_name = f\"REINVENT_round_{REINVENT_round}\"\n",
    "            \n",
    "            hitl_iteration_name = f\"HITL_iteration_{num_iters}\"\n",
    "            HITL_iteration = num_iters\n",
    "            # for HITL_iteration in range(1, num_iters+1):\n",
    "            #     hitl_iteration_name = f\"HITL_iteration_{HITL_iteration}\"    \n",
    "\n",
    "            metrics_model = {\n",
    "                'thresholds': [],\n",
    "                'TP': [], 'TN': [], 'FP': [], 'FN': [],\n",
    "                'accuracy': [], 'precision': [], 'recall': [], 'F1': [], 'MCC': [],\n",
    "            }\n",
    "\n",
    "            feedback_model_path = f\"{output_dir}/REINVENT_round_{REINVENT_round - 1}/HITL_iteration_{HITL_iteration}/{model_pretrained_name}\"\n",
    "\n",
    "            if not os.path.exists(feedback_model_path):\n",
    "                print(f\"Feedback model {feedback_model_path} does not exist.\")\n",
    "            else:\n",
    "                feedback_model = load_feedback_model(feedback_type=feedback_type, feedback_model_path=feedback_model_path)\n",
    "                predicted_scores = predict_proba_from_model_fps(feedback_type, feedback_model, features)\n",
    "\n",
    "                # Compute metrics over a range of thresholds\n",
    "                thresholds = np.linspace(0, 1, 101)\n",
    "                for threshold in thresholds:\n",
    "                    predicted_labels = (predicted_scores > threshold).astype(int)\n",
    "                    TP = np.sum((predicted_labels == 1) & (label_test == 1))\n",
    "                    TN = np.sum((predicted_labels == 0) & (label_test == 0))\n",
    "                    FP = np.sum((predicted_labels == 1) & (label_test == 0))\n",
    "                    FN = np.sum((predicted_labels == 0) & (label_test == 1))\n",
    "                    \n",
    "                    accuracy = (TP + TN) / len(label_test)\n",
    "                    precision, recall, f1, _ = precision_recall_fscore_support(label_test, predicted_labels, average='binary')\n",
    "                    mcc = matthews_corrcoef(label_test, predicted_labels)\n",
    "                    \n",
    "                    metrics_model['thresholds'].append(threshold)\n",
    "                    metrics_model['TP'].append(TP)\n",
    "                    metrics_model['TN'].append(TN)\n",
    "                    metrics_model['FP'].append(FP)\n",
    "                    metrics_model['FN'].append(FN)\n",
    "                    metrics_model['accuracy'].append(accuracy)\n",
    "                    metrics_model['precision'].append(precision)\n",
    "                    metrics_model['recall'].append(recall)\n",
    "                    metrics_model['F1'].append(f1)\n",
    "                    metrics_model['MCC'].append(mcc)\n",
    "        \n",
    "                # Compute ROC curve and AUC\n",
    "                fpr, tpr, _ = roc_curve(label_test, predicted_scores, pos_label=1)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "\n",
    "                metrics_model['fpr'] = fpr\n",
    "                metrics_model['tpr'] = tpr\n",
    "                metrics_model['roc_auc'] = roc_auc\n",
    "\n",
    "                #metrics[f\"{reinvent_round_name}_{hitl_iteration_name}\"] = metrics_model\n",
    "                metrics[reinvent_round_name] = metrics_model\n",
    "        \n",
    "    #################################\n",
    "    # DRD2 PROBABILITY DISTRIBUTION #\n",
    "    #################################\n",
    "\n",
    "    drd2_proba = {}\n",
    "\n",
    "    if benchmark[\"drd2_proba\"] == True:\n",
    "        \n",
    "        for REINVENT_round in range(1, num_rounds + 2):\n",
    "            reinvent_round_name = f\"REINVENT_round_{REINVENT_round}\"\n",
    "            \n",
    "            scaffold_memory_path = f\"{output_dir}/{reinvent_round_name}/results/scaffold_memory.csv\"\n",
    "            \n",
    "            if os.path.exists(scaffold_memory_path):\n",
    "                scaffold_df = pd.read_csv(scaffold_memory_path)\n",
    "                smiles_list_round = scaffold_df['SMILES']\n",
    "                \n",
    "                # Load the TDC Oracle\n",
    "                oracle = Oracle(name='DRD2')\n",
    "                \n",
    "                # Compute DRD2 probabilities\n",
    "                drd2_probs = [oracle(smiles) for smiles in smiles_list_round]\n",
    "                \n",
    "                # Store the results\n",
    "                drd2_proba[reinvent_round_name] = np.array(drd2_probs)\n",
    "            else:\n",
    "                print(f\"Scaffold memory file {scaffold_memory_path} does not exist.\")\n",
    "\n",
    "    ##############################\n",
    "    # ALL OTHER BENCHMARK SCORES #\n",
    "    ##############################\n",
    "\n",
    "    novelty_score = {}\n",
    "    sa_score = {}\n",
    "    qed_score = {}\n",
    "    logP = {}\n",
    "    mol_weight = {}\n",
    "    h_donors = {}\n",
    "    h_acceptors = {}\n",
    "    tpsa = {}\n",
    "    rotatable_bonds = {}\n",
    "    num_rings = {}\n",
    "    \n",
    "    if benchmark[\"mol_props\"] == True:\n",
    "        for REINVENT_round in range(1, num_rounds + 2):\n",
    "            reinvent_round_name = f\"REINVENT_round_{REINVENT_round}\"\n",
    "            scaffold_memory_path = f\"{output_dir}/{reinvent_round_name}/results/scaffold_memory.csv\"\n",
    "            \n",
    "            if os.path.exists(scaffold_memory_path):\n",
    "                scaffold_df = pd.read_csv(scaffold_memory_path)\n",
    "                smiles_list = scaffold_df['SMILES']\n",
    "                \n",
    "                novelty_scores_round = []\n",
    "                sa_scores_round = []\n",
    "                qed_score_round = []\n",
    "                logP_round = []\n",
    "                mol_weight_round = []\n",
    "                h_donors_round = []\n",
    "                h_acceptors_round = []\n",
    "                tpsa_round = []\n",
    "                rotatable_bonds_round = []\n",
    "                num_rings_round = []\n",
    "\n",
    "                for smiles in smiles_list:\n",
    "                    mol = Chem.MolFromSmiles(smiles)\n",
    "                    if mol:\n",
    "                        fps = compute_fingerprints(smiles)\n",
    "                        # Reference fps is the testing features\n",
    "                        similarities = DataStructs.BulkTanimotoSimilarity(fps, features_list)\n",
    "                        max_similarity = max(similarities)\n",
    "                        novelty = 1 - max_similarity\n",
    "                        novelty_scores_round.append(novelty)\n",
    "                        sa_scores_round.append(sascorer.calculateScore(mol))\n",
    "                        qed_score_round.append(Chem.QED.qed(mol))\n",
    "                        logP_round.append(Crippen.MolLogP(mol))\n",
    "                        mol_weight_round.append(Chem.Descriptors.MolWt(mol))\n",
    "                        h_donors_round.append(Chem.Descriptors.NumHDonors(mol))\n",
    "                        h_acceptors_round.append(Chem.Descriptors.NumHAcceptors(mol))\n",
    "                        tpsa_round.append(Chem.rdMolDescriptors.CalcTPSA(mol))\n",
    "                        rotatable_bonds_round.append(Chem.rdMolDescriptors.CalcNumRotatableBonds(mol))\n",
    "                        num_rings_round.append(Chem.rdMolDescriptors.CalcNumRings(mol))\n",
    "                    else:\n",
    "                        novelty_scores_round.append(None)\n",
    "                        sa_scores_round.append(None)\n",
    "                        qed_score_round.append(None)\n",
    "                        logP_round.append(None)\n",
    "                        mol_weight_round.append(None)\n",
    "                        h_donors_round.append(None)\n",
    "                        h_acceptors_round.append(None)\n",
    "                        tpsa_round.append(None)\n",
    "                        rotatable_bonds_round.append(None)\n",
    "                        num_rings_round.append(None)   \n",
    "                # Store the results\n",
    "                novelty_score[reinvent_round_name] = np.array(novelty_scores_round)\n",
    "                sa_score[reinvent_round_name] = np.array(sa_scores_round)\n",
    "                qed_score[reinvent_round_name] = np.array(qed_score_round)\n",
    "                logP[reinvent_round_name] = np.array(logP_round)\n",
    "                mol_weight[reinvent_round_name] = np.array(mol_weight_round)\n",
    "                h_donors[reinvent_round_name] = np.array(h_donors_round)\n",
    "                h_acceptors[reinvent_round_name] = np.array(h_acceptors_round)\n",
    "                tpsa[reinvent_round_name] = np.array(tpsa_round)\n",
    "                rotatable_bonds[reinvent_round_name] = np.array(rotatable_bonds_round)\n",
    "                num_rings[reinvent_round_name] = np.array(num_rings_round)\n",
    "            else:\n",
    "                print(f\"Scaffold memory file {scaffold_memory_path} does not exist.\")\n",
    "\n",
    "    return smiles_list_dict, metrics, drd2_proba, novelty_score, sa_score, qed_score,\\\n",
    "            logP, mol_weight, h_donors, h_acceptors,\\\n",
    "            tpsa, rotatable_bonds, num_rings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running score regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n"
     ]
    }
   ],
   "source": [
    "feedback_type = \"scoring\" \n",
    "\n",
    "# feedback type as scoring:\n",
    "# Given a molecule, what is the probability that the molecule is active regarding DRD2?  \n",
    "\n",
    "num_rounds = 3 # number of rounds, corresponding to R in the paper\n",
    "num_iters = 5 # number of iterations of showing molecules to the human for feedback at each round\n",
    "num_queries = 56 # number of molecules, pairs or a set of molecules, dependig on the task, \n",
    "                 # shown to the simulated chemist at each HITL_iteration\n",
    "\n",
    "for acquisition in [\"random\", \"uncertainty\", \"greedy\"]:\n",
    "    for sigma_noise in [0.0, 0.1]:\n",
    "        output_dir = f\"output_score_regression_n_steps_100/R{num_rounds}_T{num_iters}_Q{num_queries}_acq_{acquisition}_noise_{sigma_noise}\"\n",
    "        initial_feedback_model_path = f\"/home/springnuance/reinvent-hitl/Base-Code-Binh/training_Score_Regression_model/score_regression_model.pth\"\n",
    "        final_testing_dataset_path = \"/home/springnuance/reinvent-hitl/Base-Code-Binh/data/drd2_final_test_large.csv\"\n",
    "\n",
    "        model_pretrained_name = \"score_regression_model.pth\"\n",
    "\n",
    "        benchmark = {\n",
    "            \"smiles_list\": True,\n",
    "            \"metrics\": False,\n",
    "            \"drd2_proba\": False,\n",
    "            \"mol_props\": False,\n",
    "        }\n",
    "\n",
    "        smiles_list, metrics, drd2_proba, novelty_score, sa_score, qed_score,\\\n",
    "            logP, mol_weight, h_donors, h_acceptors,\\\n",
    "            tpsa, rotatable_bonds, num_rings = evaluate_results(\n",
    "                output_dir, benchmark,\n",
    "                feedback_type, # scoring, comparing, ranking\n",
    "                initial_feedback_model_path,\n",
    "                final_testing_dataset_path, # path to the final training dataset\n",
    "                model_pretrained_name, # Name of the pretrained model before REINVENT_round_1\n",
    "                num_rounds, # number of rounds, corresponding to R in the paper\n",
    "                num_iters, # number of iterations of showing molecules to the human for feedback at each round, corresponding to T in the paper\n",
    "        )\n",
    "\n",
    "        np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_smiles_list.npy\", smiles_list)\n",
    "        if benchmark[\"metrics\"] == True:\n",
    "            np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_metrics.npy\", metrics)\n",
    "        if benchmark[\"drd2_proba\"] == True:\n",
    "            np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_drd2_proba.npy\", drd2_proba)\n",
    "        if benchmark[\"mol_props\"] == True:\n",
    "            np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_novelty_score.npy\", novelty_score)\n",
    "            np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_sa_score.npy\", sa_score)\n",
    "            np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_qed_score.npy\", qed_score)\n",
    "            np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_logP.npy\", logP)\n",
    "            np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_mol_weight.npy\", mol_weight)\n",
    "            np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_h_donors.npy\", h_donors)\n",
    "            np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_h_acceptors.npy\", h_acceptors)\n",
    "            np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_tpsa.npy\", tpsa)\n",
    "            np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_rotatable_bonds.npy\", rotatable_bonds)\n",
    "            np.save(f\"results_score_regression_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_num_rings.npy\", num_rings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Bradley Terry model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n"
     ]
    }
   ],
   "source": [
    "feedback_type = \"comparing\" # scoring, comparing, ranking\n",
    "\n",
    "# feedback type as comparing:\n",
    "# Given two molecules, what is the probability that the first molecule is more active than the second molecule regarding DRD2?\n",
    "\n",
    "num_rounds = 3 # number of rounds, corresponding to R in the paper\n",
    "num_iters = 5 # number of iterations of showing molecules to the human for feedback at each round\n",
    "num_queries = 8 # number of molecules, pairs or a set of molecules, dependig on the task, \n",
    "                 # shown to the simulated chemist at each HITL_iteration\n",
    "\n",
    "for acquisition in [\"random\", \"uncertainty\", \"greedy\"]:\n",
    "    for sigma_noise in [0.0, 0.1]:\n",
    "        output_dir = f\"output_bradley_terry_n_steps_100/R{num_rounds}_T{num_iters}_Q{num_queries}_acq_{acquisition}_noise_{sigma_noise}\"\n",
    "        initial_feedback_model_path = f\"/home/springnuance/reinvent-hitl/Base-Code-Binh/training_Bradley_Terry_model/bradley_terry_model.pth\"\n",
    "        final_testing_dataset_path = \"/home/springnuance/reinvent-hitl/Base-Code-Binh/data/drd2_final_test_large.csv\"\n",
    "\n",
    "        model_pretrained_name = \"bradley_terry_model.pth\"\n",
    "\n",
    "\n",
    "        benchmark = {\n",
    "            \"smiles_list\": True,\n",
    "            \"metrics\": False,\n",
    "            \"drd2_proba\": False,\n",
    "            \"mol_props\": False,\n",
    "        }\n",
    "\n",
    "        smiles_list, metrics, drd2_proba, novelty_score, sa_score, qed_score,\\\n",
    "            logP, mol_weight, h_donors, h_acceptors,\\\n",
    "            tpsa, rotatable_bonds, num_rings = evaluate_results(\n",
    "                output_dir, benchmark,\n",
    "                feedback_type, # scoring, comparing, ranking\n",
    "                initial_feedback_model_path,\n",
    "                final_testing_dataset_path, # path to the final training dataset\n",
    "                model_pretrained_name, # Name of the pretrained model before REINVENT_round_1\n",
    "                num_rounds, # number of rounds, corresponding to R in the paper\n",
    "                num_iters, # number of iterations of showing molecules to the human for feedback at each round, corresponding to T in the paper\n",
    "        )\n",
    "\n",
    "        np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_smiles_list.npy\", smiles_list)\n",
    "        if benchmark[\"metrics\"] == True:\n",
    "            np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_metrics.npy\", metrics)\n",
    "        if benchmark[\"drd2_proba\"] == True:\n",
    "            np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_drd2_proba.npy\", drd2_proba)\n",
    "        if benchmark[\"mol_props\"] == True:\n",
    "            np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_novelty_score.npy\", novelty_score)\n",
    "            np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_sa_score.npy\", sa_score)\n",
    "            np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_qed_score.npy\", qed_score)\n",
    "            np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_logP.npy\", logP)\n",
    "            np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_mol_weight.npy\", mol_weight)\n",
    "            np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_h_donors.npy\", h_donors)\n",
    "            np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_h_acceptors.npy\", h_acceptors)\n",
    "            np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_tpsa.npy\", tpsa)\n",
    "            np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_rotatable_bonds.npy\", rotatable_bonds)\n",
    "            np.save(f\"results_bradley_terry_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_num_rings.npy\", num_rings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Rank ListNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n",
      "Loading final testing dataset successfully\n"
     ]
    }
   ],
   "source": [
    "feedback_type = \"ranking\" # scoring, comparing, ranking\n",
    "\n",
    "# feedback type as ranking:\n",
    "# Given N molecules, what are the orders of preference of these molecules regarding DRD2?\n",
    "\n",
    "num_rounds = 3 # number of rounds, corresponding to R in the paper\n",
    "num_iters = 5 # number of iterations of showing molecules to the human for feedback at each round\n",
    "num_queries = 8 # number of molecules, pairs or a set of molecules, depending on the task, \n",
    "                 # shown to the simulated chemist at each HITL_iteration\n",
    "\n",
    "for acquisition in [\"random\", \"uncertainty\", \"greedy\"]:\n",
    "    for sigma_noise in [0.0, 0.1]:\n",
    "        output_dir = f\"output_rank_listnet_n_steps_100/R{num_rounds}_T{num_iters}_Q{num_queries}_acq_{acquisition}_noise_{sigma_noise}\"\n",
    "        initial_feedback_model_path = f\"/home/springnuance/reinvent-hitl/Base-Code-Binh/training_Rank_ListNet_model/rank_listnet_model.pth\"\n",
    "        final_testing_dataset_path = \"/home/springnuance/reinvent-hitl/Base-Code-Binh/data/drd2_final_test_large.csv\"\n",
    "\n",
    "        model_pretrained_name = \"rank_listnet_model.pth\"\n",
    "\n",
    "\n",
    "        benchmark = {\n",
    "            \"smiles_list\": True,\n",
    "            \"metrics\": False,\n",
    "            \"drd2_proba\": False,\n",
    "            \"mol_props\": False,\n",
    "        }\n",
    "\n",
    "        smiles_list, metrics, drd2_proba, novelty_score, sa_score, qed_score,\\\n",
    "            logP, mol_weight, h_donors, h_acceptors,\\\n",
    "            tpsa, rotatable_bonds, num_rings = evaluate_results(\n",
    "                output_dir, benchmark,\n",
    "                feedback_type, # scoring, comparing, ranking\n",
    "                initial_feedback_model_path,\n",
    "                final_testing_dataset_path, # path to the final training dataset\n",
    "                model_pretrained_name, # Name of the pretrained model before REINVENT_round_1\n",
    "                num_rounds, # number of rounds, corresponding to R in the paper\n",
    "                num_iters, # number of iterations of showing molecules to the human for feedback at each round, corresponding to T in the paper\n",
    "        )\n",
    "\n",
    "        np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_smiles_list.npy\", smiles_list)\n",
    "        if benchmark[\"metrics\"] == True:\n",
    "            np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_metrics.npy\", metrics)\n",
    "        if benchmark[\"drd2_proba\"] == True:\n",
    "            np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_drd2_proba.npy\", drd2_proba)\n",
    "        if benchmark[\"mol_props\"] == True:\n",
    "            np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_novelty_score.npy\", novelty_score)\n",
    "            np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_sa_score.npy\", sa_score)\n",
    "            np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_qed_score.npy\", qed_score)\n",
    "            np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_logP.npy\", logP)\n",
    "            np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_mol_weight.npy\", mol_weight)\n",
    "            np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_h_donors.npy\", h_donors)\n",
    "            np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_h_acceptors.npy\", h_acceptors)\n",
    "            np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_tpsa.npy\", tpsa)\n",
    "            np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_rotatable_bonds.npy\", rotatable_bonds)\n",
    "            np.save(f\"results_rank_listnet_n_steps_100/acq_{acquisition}_noise_{sigma_noise}_num_rings.npy\", num_rings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc_env_hitl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
