{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rdkit.Chem as Chem\n",
    "from numpy.random import default_rng\n",
    "from ast import literal_eval\n",
    "\n",
    "from utils import fingerprints_from_mol\n",
    "from scripts.simulated_expert import ActivityEvaluationModel, logPEvaluationModel\n",
    "from scripts.write_config import write_REINVENT_config, write_sample_file\n",
    "from models.RandomForest import RandomForestReg, RandomForestClf\n",
    "from scripts.acquisition import select_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we need to install the custom reinvent scoring package to support the Bradley-Terry model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: reinvent-scoring\n",
      "Version: 0.0.73\n",
      "Summary: Scoring functions for Reinvent\n",
      "Home-page: https://github.com/MolecularAI/reinvent-scoring.git\n",
      "Author: MolecularAI\n",
      "Author-email: patronov@gmail.com\n",
      "License: UNKNOWN\n",
      "Location: /home/springnuance/reinvent-hitl/reinvent_scoring\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show reinvent_scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there already exists reinvent_scoring, we should uninstall it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: reinvent-scoring 0.0.73\n",
      "Can't uninstall 'reinvent-scoring'. No files were found to uninstall.\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall -y reinvent_scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we install the custom reinvent scoring package\n",
    "### The flag -e means that the package is installed in editable mode, so that changes to the code will be immediately available without reinstalling the package. All package info is stored in the setup.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/springnuance/reinvent-hitl/reinvent_scoring\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: reinvent-scoring-bradley-terry\n",
      "  Attempting uninstall: reinvent-scoring-bradley-terry\n",
      "    Found existing installation: reinvent-scoring-bradley-terry 0.0.73\n",
      "    Uninstalling reinvent-scoring-bradley-terry-0.0.73:\n",
      "      Successfully uninstalled reinvent-scoring-bradley-terry-0.0.73\n",
      "  Running setup.py develop for reinvent-scoring-bradley-terry\n",
      "Successfully installed reinvent-scoring-bradley-terry-0.0.73\n"
     ]
    }
   ],
   "source": [
    "! pip install -e \"../reinvent_scoring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyTDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ECFP_dataset(init_train_set_path, num_train_samples):\n",
    "    \"\"\"\n",
    "        Load background training data used to pre-train the predictive model    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading D0\")\n",
    "    train_set = pd.read_csv(init_train_set_path)\n",
    "    feature_cols = [f\"bit{i}\" for i in range(2048)]\n",
    "    target_col = [\"activity\"]\n",
    "    smiles_train = train_set[\"smiles\"].values.reshape(-1)\n",
    "    x_train = train_set[feature_cols].values\n",
    "    y_train = train_set[target_col].values.reshape(-1)\n",
    "    sample_weight = np.array([1. for i in range(len(x_train))])\n",
    "    print(\"The feature matrix shape: \", x_train.shape)\n",
    "    print(\"The labels shape: \", y_train.shape)\n",
    "\n",
    "    train_sample = train_set[train_set[\"activity\"] == 1].sample(num_train_samples).smiles.tolist()\n",
    "    return x_train, y_train, sample_weight, smiles_train, train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_HITL(\n",
    "        seed, dirname, reinvent_dir, reinvent_env,\n",
    "        init_model_path, \n",
    "        init_train_set_path, \n",
    "        model_type, # either \"regression\" or \"classification\"\n",
    "        num_rounds, # number of rounds, corresponding to R in the paper\n",
    "        num_iters, # number of molecules shown at each iteration to the human for feedback, corresponding to T in the paper\n",
    "        num_queries, # number of molecules shown to the simulated chemist at each iteration\n",
    "        num_train_samples, # number of training samples to select from the training set\n",
    "        REINVENT_n_steps, # number of REINVENT optimization steps\n",
    "        train_similarity, # if True, use the similarity of the training set to select queries\n",
    "        pretrained_prior, # if True, use a pre-trained prior\n",
    "        acquisition, # acquisition: 'uncertainty', 'random', 'thompson', 'greedy' (if None run with no human interaction)\n",
    "        sigma_noise, # noise level for simulated chemist's responses\n",
    "        threshold # threshold for high scoring molecules\n",
    "        ):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    rng = default_rng(seed)\n",
    "\n",
    "    if acquisition:\n",
    "        jobname = \"fine-tune predictive component HITL\"\n",
    "        jobid = f\"{dirname}_rounds_{num_rounds}_iters_{num_iters}_queries_{num_queries}_{acquisition}_noise_{sigma_noise}\"\n",
    "        output_dir = f\"{jobid}_seed_{seed}\"\n",
    "    else:\n",
    "        jobname = \"fine-tune predictive component no HITL\"\n",
    "        jobid = f\"{dirname}_rounds_{num_rounds}_None\"\n",
    "        output_dir = f\"{jobid}_seed_{seed}\"\n",
    "    \n",
    "    # initial configuration\n",
    "    conf_filename = \"config.json\"\n",
    "\n",
    "    # create root output dir\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "    print(f\"Creating output directory: {output_dir}.\")\n",
    "    configuration_JSON_path = write_REINVENT_config(reinvent_dir, reinvent_env, output_dir, conf_filename, jobid, jobname)\n",
    "    print(f\"Creating config file: {configuration_JSON_path}.\")\n",
    "\n",
    "    configuration = json.load(open(os.path.join(output_dir, conf_filename)))\n",
    "    print(configuration)\n",
    "    # load background training data used to pre-train the predictive model\n",
    "    x_train, y_train, sample_weight, smiles_train, train_sample = extract_ECFP_dataset(init_train_set_path, num_train_samples)\n",
    "\n",
    "    \n",
    "    print(\"The training similarity used to select queries: \", train_similarity)\n",
    "    print(\"The number of training samples: \", len(train_sample))\n",
    "\n",
    "    # write specified number of RL optimization steps in configuration\n",
    "    # (example: if num_rounds = 5 (rounds) and Reinvent REINVENT_n_steps = 100, we will do 5*100 RL optimization steps)\n",
    "    configuration[\"parameters\"][\"reinforcement_learning\"][\"n_steps\"] = REINVENT_n_steps\n",
    "    #print(configuration)\n",
    "\n",
    "    # write initial model path in configuration\n",
    "    configuration_scoring_function = configuration[\"parameters\"][\"scoring_function\"][\"parameters\"]\n",
    "    print(configuration_scoring_function)\n",
    "    for i in range(len(configuration_scoring_function)):\n",
    "        if configuration_scoring_function[i][\"component_type\"] == \"predictive_property\":\n",
    "            configuration_scoring_function[i][\"specific_parameters\"][\"model_path\"] = init_model_path\n",
    "            configuration_scoring_function[i][\"specific_parameters\"][\"scikit\"] = model_type\n",
    "            if model_type == \"classification\": \n",
    "                configuration_scoring_function[i][\"specific_parameters\"][\"transformation\"] = {\"transformation_type\": \"no_transformation\"}\n",
    "        if configuration_scoring_function[i][\"component_type\"] == \"tanimoto_similarity\":\n",
    "           configuration_scoring_function[i][\"specific_parameters\"][\"smiles\"] = train_sample \n",
    "    if pretrained_prior:\n",
    "       configuration[\"parameters\"][\"reinforcement_learning\"][\"agent\"] = \"/home/klgx638/Generations/HITL_qsar_experiments_final/priors/logp/focused.agent\"\n",
    "            \n",
    "    \n",
    "    # write the updated configuration file to the disc\n",
    "    configuration_JSON_path = os.path.join(output_dir, conf_filename)\n",
    "    with open(configuration_JSON_path, 'w') as f:\n",
    "        json.dump(configuration, f, indent=4, sort_keys=True)\n",
    "\n",
    "    # initialize the active learning with the same pool of generated compounds resulting from a standard Reinvent run\n",
    "    initial_dir = f\"{dirname}_rounds_{num_rounds}_None_seed_{seed}\"\n",
    "    if os.path.exists(initial_dir): # if you already have a standard Reinvent run\n",
    "        # copy the file containing the initial unlabelled pool in your current directory\n",
    "        os.makedirs(os.path.join(output_dir, \"iteration_0\"))\n",
    "        try:\n",
    "            initial_unlabelled_pool = os.path.join(initial_dir, \"results/scaffold_memory.csv\")\n",
    "            shutil.copy(initial_unlabelled_pool, os.path.join(output_dir, \"iteration_0\"))\n",
    "        # if this file does not exist, skip this step\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    else: # if you do not have a standard Reinvent run, skip this step\n",
    "        pass\n",
    "\n",
    "    print(f\"Running MPO experiment with rounds {num_rounds}, iters {num_iters}, queries {num_queries}, seed {seed}. \\n Results will be saved at {output_dir}\")\n",
    "\n",
    "    # initialize human feedback model, this \n",
    "    # Loading the bradley terry model using Torch\n",
    "    feedback_model = ... \n",
    "    print(\"Loading feedback model.\")\n",
    "\n",
    "    # load the predictive model\n",
    "    predictive_model_name = init_model_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    print(\"The predictive model name: \", predictive_model_name)\n",
    "    model_load_path = output_dir + '/{}_iteration_0.pkl'.format(predictive_model_name)\n",
    "    if not os.path.exists(model_load_path):\n",
    "        shutil.copy(init_model_path, output_dir)\n",
    "    fitted_model = pickle.load(open(init_model_path, 'rb'))\n",
    "    print(\"Loading predictive model.\")\n",
    "\n",
    "    # store expert scores\n",
    "    expert_score = []\n",
    "\n",
    "    READ_ONLY = False # if folder exists, do not overwrite results there\n",
    "\n",
    "    for REINVENT_round in np.arange(1, num_rounds + 1):\n",
    "\n",
    "        if REINVENT_round == 1 and acquisition:\n",
    "            if os.path.exists(os.path.join(output_dir, \"iteration_0/scaffold_memory.csv\")):\n",
    "                # start from your pre-existing pool of unlabelled compounds\n",
    "                with open(os.path.join(output_dir, \"iteration_0/scaffold_memory.csv\"), 'r') as file:\n",
    "                    data = pd.read_csv(file)\n",
    "                data = data[data[\"Step\"] < 100]\n",
    "                data.reset_index(inplace=True)\n",
    "            else:\n",
    "                # generate a pool of unlabelled compounds with REINVENT\n",
    "                print(\"Run REINVENT\")\n",
    "                os.system(reinvent_env + '/bin/python ' + reinvent_dir + '/input.py ' + configuration_JSON_path + '&> ' + output_dir + '/run.err')\n",
    "                \n",
    "                with open(os.path.join(output_dir, \"results/scaffold_memory.csv\"), 'r') as file:\n",
    "                    data = pd.read_csv(file)\n",
    "\n",
    "        else:\n",
    "            if(not READ_ONLY):\n",
    "                # run REINVENT\n",
    "                print(\"Run REINVENT\")\n",
    "                os.system(reinvent_env + '/bin/python ' + reinvent_dir + '/input.py ' + configuration_JSON_path + '&> ' + output_dir + '/run.err')\n",
    "            else:\n",
    "                print(\"Reading REINVENT results from file, no re-running.\")\n",
    "                pass\n",
    "\n",
    "            with open(os.path.join(output_dir, \"results/scaffold_memory.csv\"), 'r') as file:\n",
    "                data = pd.read_csv(file)\n",
    "        \n",
    "        N = len(data)\n",
    "        colnames = list(data) \n",
    "        smiles = data['SMILES']\n",
    "        bioactivity_score = data['bioactivity'] # the same as raw_bioactivity since no transformation applied\n",
    "        raw_bioactivity_score = data['raw_bioactivity']\n",
    "        high_scoring_threshold = threshold\n",
    "        # save the indexes of high scoring molecules for bioactivity\n",
    "        high_scoring_idx = bioactivity_score > high_scoring_threshold\n",
    "\n",
    "        # Scoring component values\n",
    "        scoring_component_names = [s.split(\"raw_\")[1] for s in colnames if \"raw_\" in s]\n",
    "        print(f\"scoring components: {scoring_component_names}\")\n",
    "        x = np.array(data[scoring_component_names])\n",
    "        print(f'Scoring component matrix dimensions: {x.shape}')\n",
    "        x = x[high_scoring_idx,:]\n",
    "\n",
    "        # Only analyse highest scoring molecules\n",
    "        smiles = smiles[high_scoring_idx]\n",
    "        bioactivity_score = bioactivity_score[high_scoring_idx]\n",
    "        raw_bioactivity_score = raw_bioactivity_score[high_scoring_idx]\n",
    "        print(f'{len(smiles)} high-scoring (> {high_scoring_threshold}) molecules')\n",
    "\n",
    "        if len(smiles) == 0:\n",
    "            smiles = data['SMILES']\n",
    "            print(f'{len(smiles)} molecules')\n",
    "\n",
    "               \n",
    "        # store molecule indexes selected for feedback\n",
    "        selected_feedback = np.empty(0).astype(int)\n",
    "        human_sample_weight = np.empty(0).astype(float)\n",
    "        # store number of accepted queries (y = 1) at each iteration\n",
    "        n_accept = []\n",
    "\n",
    "        ########################### HITL rounds ######################################\n",
    "        \n",
    "        for iteration in np.arange(num_iters): # T number of HITL iterations\n",
    "            print(f\"Round = {REINVENT_round}, Iteration = {iteration}\")\n",
    "            # query selection\n",
    "            if model_type == \"regression\":\n",
    "                model = RandomForestReg(fitted_model)\n",
    "            if model_type == \"classification\":\n",
    "                model = RandomForestClf(fitted_model)\n",
    "            if len(smiles) > num_queries:\n",
    "                new_query = select_query(data, num_queries, list(smiles), model, selected_feedback, acquisition, rng) # select n smiles with Active Learning\n",
    "            else:\n",
    "                new_query = select_query(data, len(smiles), list(smiles), model, selected_feedback, acquisition, rng)\n",
    "            \n",
    "            # Initialize the expert values vector\n",
    "            s_bioactivity = [] # for scores (between 0 and 1)\n",
    "            v_bioactivity = [] # for continuous feedback (regression)\n",
    "            # Get expert feedback on selected queries\n",
    "            print(new_query)\n",
    "            for i in new_query:\n",
    "                cur_mol = data.iloc[i][\"SMILES\"]\n",
    "                print(cur_mol)\n",
    "                value = feedback_model.human_score(cur_mol, sigma_noise)\n",
    "                s_bioactivity.append(value)\n",
    "                if model_type == \"regression\":\n",
    "                    v_bioactivity.append(feedback_model.utility(value, low = 2, high = 4))\n",
    "            \n",
    "            # Get raw scores and transformed score (if any) from the high scoring molecules in U\n",
    "            raw_scoring_component_names = [\"raw_\"+name for name in scoring_component_names] \n",
    "            x_raw = data[raw_scoring_component_names].to_numpy()\n",
    "            x =  data[scoring_component_names].to_numpy()\n",
    "\n",
    "            # get (binary) simulated chemist's responses\n",
    "            if model_type == \"regression\":\n",
    "                new_y = np.array(v_bioactivity)\n",
    "                s_bioactivity = [1 if s > 0.5 else 0 for s in v_bioactivity]\n",
    "                accepted = s_bioactivity\n",
    "            if model_type == \"classification\":\n",
    "                new_y = np.array([1 if s > 0.5 else 0 for s in s_bioactivity])\n",
    "                accepted = new_y.tolist()\n",
    "            expert_score += [accepted]\n",
    "            n_accept += [sum(accepted)]\n",
    "\n",
    "            print(f\"Feedback idx at iteration {REINVENT_round}, {iteration}: {new_query}\")\n",
    "            print(f\"Number of accepted molecules at iteration {REINVENT_round}, {iteration}: {n_accept[iteration]}\")   \n",
    "            \n",
    "            # append feedback\n",
    "            if len(new_y) > 0:\n",
    "                selected_feedback = np.hstack((selected_feedback, new_query))\n",
    "\n",
    "            mask = np.ones(N, dtype=bool)\n",
    "            mask[selected_feedback] = False\n",
    "\n",
    "            # use the augmented training data to retrain the model\n",
    "            new_smiles = data.iloc[new_query].SMILES.tolist()\n",
    "            new_mols = [Chem.MolFromSmiles(s) for s in new_smiles]\n",
    "            new_x = fingerprints_from_mol(new_mols, type = \"counts\")\n",
    "            new_human_sample_weight = np.array([s if s > 0.5 else 1-s for s in s_bioactivity])\n",
    "            sample_weight = np.concatenate([sample_weight, new_human_sample_weight])\n",
    "            print(len(new_x), len(new_y))\n",
    "            x_train = np.concatenate([x_train, new_x])\n",
    "            y_train = np.concatenate([y_train, new_y])\n",
    "            smiles_train = np.concatenate([smiles_train, new_smiles])\n",
    "            print(f\"Augmented train set size at iteration {REINVENT_round}: {x_train.shape[0]} {y_train.shape[0]}\")\n",
    "            # save augmented training data\n",
    "            D_r = pd.DataFrame(np.concatenate([smiles_train.reshape(-1,1), x_train, y_train.reshape(-1,1)], 1))\n",
    "            D_r.columns = [\"SMILES\"] + [f\"bit{i}\" for i in range(x_train.shape[1])] + [\"target\"]\n",
    "            D_r.to_csv(os.path.join(output_dir, f\"augmented_train_set_iter{REINVENT_round}.csv\"))\n",
    "\n",
    "            # re-fit and save the model using the augmented train set and save to new directory\n",
    "            model_new_savefile = output_dir + '/{}_iteration_{}.pkl'.format(predictive_model_name, REINVENT_round)\n",
    "            model._retrain(x_train, y_train, sample_weight = sample_weight, save_to_path = model_new_savefile)\n",
    "            fitted_model = pickle.load(open(model_new_savefile, 'rb'))\n",
    "\n",
    "            # get current configuration\n",
    "            configuration = json.load(open(os.path.join(output_dir, conf_filename)))\n",
    "            conf_filename = \"iteration{}_config.json\".format(REINVENT_round)    \n",
    "\n",
    "            # modify model path in configuration\n",
    "            configuration_scoring_function = configuration[\"parameters\"][\"scoring_function\"][\"parameters\"]\n",
    "            for i in range(len(configuration_scoring_function)):\n",
    "                if configuration_scoring_function[i][\"component_type\"] == \"predictive_property\":\n",
    "                    configuration_scoring_function[i][\"specific_parameters\"][\"model_path\"] = model_new_savefile\n",
    "\n",
    "            # Keep agent checkpoint\n",
    "            if REINVENT_round == 1:\n",
    "                configuration[\"parameters\"][\"reinforcement_learning\"][\"agent\"] = os.path.join(initial_dir, \"results/Agent.ckpt\")\n",
    "            else:\n",
    "                configuration[\"parameters\"][\"reinforcement_learning\"][\"agent\"] = os.path.join(output_dir, \"results/Agent.ckpt\")\n",
    "\n",
    "        root_output_dir = os.path.expanduser(\"{}_seed{}\".format(jobid, seed))\n",
    "\n",
    "        # Define new directory for the next round\n",
    "        output_dir = os.path.join(root_output_dir, \"iteration{}_{}\".format(REINVENT_round, acquisition))\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        print(output_dir)\n",
    "\n",
    "        # modify log and result paths in configuration\n",
    "        configuration[\"logging\"][\"logging_path\"] = os.path.join(output_dir, \"progress.log\")\n",
    "        configuration[\"logging\"][\"result_folder\"] = os.path.join(output_dir, \"results\")\n",
    "\n",
    "        # write the updated configuration file to the disc\n",
    "        configuration_JSON_path = os.path.join(output_dir, conf_filename)\n",
    "        with open(configuration_JSON_path, 'w') as f:\n",
    "            json.dump(configuration, f, indent=4, sort_keys=True)\n",
    "\n",
    "    r = np.arange(len(expert_score))\n",
    "    m_score = [np.mean(expert_score[i]) for i in r]\n",
    "    print(\"Mean expert score : \", m_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/springnuance/reinvent-hitl/Base-Code-Binh\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output directory: outputs_rounds_2_iters_10_queries_10_thompson_noise_0.0_seed_42.\n",
      "Creating config file: outputs_rounds_2_iters_10_queries_10_thompson_noise_0.0_seed_42/config.json.\n",
      "{'logging': {'job_id': 'outputs_rounds_2_iters_10_queries_10_thompson_noise_0.0', 'job_name': 'fine-tune predictive component HITL', 'logging_frequency': 0, 'logging_path': 'outputs_rounds_2_iters_10_queries_10_thompson_noise_0.0_seed_42/progress.log', 'recipient': 'local', 'result_folder': 'outputs_rounds_2_iters_10_queries_10_thompson_noise_0.0_seed_42/results', 'sender': 'http://127.0.0.1'}, 'model_type': 'default', 'parameters': {'diversity_filter': {'bucket_size': 25, 'minscore': 0.2, 'minsimilarity': 0.4, 'name': 'IdenticalMurckoScaffold'}, 'inception': {'memory_size': 20, 'sample_size': 5, 'smiles': []}, 'reinforcement_learning': {'agent': '../Reinvent/data/random.prior.new', 'batch_size': 128, 'learning_rate': 0.0001, 'margin_threshold': 50, 'n_steps': 250, 'prior': '../Reinvent/data/random.prior.new', 'reset': 0, 'reset_score_cutoff': 0.5, 'sigma': 128}, 'scoring_function': {'name': 'custom_sum', 'parallel': False, 'parameters': [{'component_type': 'predictive_property', 'name': 'Human-Component', 'specific_parameters': {'bradley_terry': 'classification', 'container_type': 'bradley_terry_container', 'descriptor_type': 'ecfp', 'model_path': 'models/bradley_terry_model.pth', 'size': 2048, 'transformation': {'transformation_type': 'no_transformation'}, 'use_counts': True, 'use_features': True}, 'weight': 1}]}}, 'run_type': 'reinforcement_learning', 'version': 3}\n",
      "Loading D0\n",
      "The feature matrix shape:  (7101, 2048)\n",
      "The labels shape:  (7101,)\n",
      "The training similarity used to select queries:  False\n",
      "The number of training samples:  30\n",
      "[{'component_type': 'predictive_property', 'name': 'Human-Component', 'specific_parameters': {'bradley_terry': 'classification', 'container_type': 'bradley_terry_container', 'descriptor_type': 'ecfp', 'model_path': 'models/bradley_terry_model.pth', 'size': 2048, 'transformation': {'transformation_type': 'no_transformation'}, 'use_counts': True, 'use_features': True}, 'weight': 1}]\n",
      "Running MPO experiment with rounds 2, iters 10, queries 10, seed 42. \n",
      " Results will be saved at outputs_rounds_2_iters_10_queries_10_thompson_noise_0.0_seed_42\n",
      "Loading feedback model.\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "A load persistent id instruction was encountered,\nbut no persistent_load function was specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3abb599304d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m          \u001b[0mtrain_similarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_prior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m          \u001b[0macquisition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_noise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m          benchmark, threshold)\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-2d0e5ae75aea>\u001b[0m in \u001b[0;36mrun_HITL\u001b[0;34m(seed, dirname, reinvent_dir, reinvent_env, init_model_path, init_train_set_path, model_type, num_rounds, num_iters, num_queries, num_train_samples, REINVENT_n_steps, train_similarity, pretrained_prior, acquisition, sigma_noise, benchmark, threshold)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_load_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mfitted_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading predictive model.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: A load persistent id instruction was encountered,\nbut no persistent_load function was specified."
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "dirname = \"outputs\"\n",
    "\n",
    "# change these path variables as required\n",
    "reinvent_dir = os.path.expanduser(\"/home/springnuance/reinvent-hitl/Reinvent\") # We must use absolute path\n",
    "reinvent_env = os.path.expanduser(\"/home/springnuance/miniconda3/envs/cc_env_reinvent\") # We must use absolute path\n",
    "\n",
    "init_model_path = \"models/bradley_terry_model.pth\" \n",
    "# Please check the performance of this bradley terry model\n",
    "# the performance should not be good, around random accuracy \n",
    "# If the model is too good, retrain the model to become weaker, we are trying to learn the model  \n",
    "\n",
    "init_train_set_path = \"/home/springnuance/reinvent-hitl/Base-Code-Binh/data/drd2/small_drd2_rank_data.csv\" \n",
    "\n",
    "model_type = \"classification\" # either \"regression\" or \"classification\"\n",
    "num_rounds = 2 # number of rounds\n",
    "REINVENT_n_steps = 100 # number of REINVENT optimization steps\n",
    "train_similarity = False # if True, use the similarity of the training set to select queries\n",
    "pretrained_prior = False # if True, use a pre-trained prior\n",
    "\n",
    "\n",
    "# Please look at the thompson sampling code and fix it!\n",
    "acquisition = \"thompson\" # acquisition: 'uncertainty', 'random', 'thompson', 'greedy' (if None run with no human interaction)\n",
    "\n",
    "sigma_noise = 0.0 # noise level for simulated chemist's responses\n",
    "num_iters = 10 # number of molecules shown at each iteration to the human for feedback\n",
    "num_queries = 10 # number of molecules shown to the simulated chemist at each iteration (10 pairs)\n",
    "num_train_samples = 30 # number of training samples to select from the training set, ignore if train_similarity is False\n",
    "threshold = 0.5 # threshold for high scoring molecules\n",
    "\n",
    "run_HITL(seed, dirname, reinvent_dir, reinvent_env,\n",
    "         init_model_path, init_train_set_path, \n",
    "         model_type, num_rounds, \n",
    "         num_iters, num_queries, \n",
    "         num_train_samples,\n",
    "         REINVENT_n_steps, \n",
    "         train_similarity, pretrained_prior, \n",
    "         acquisition, sigma_noise, \n",
    "         threshold)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc_env_hitl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
