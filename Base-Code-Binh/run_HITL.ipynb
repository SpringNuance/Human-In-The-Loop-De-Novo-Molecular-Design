{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Please run this code with the kernel reinvent.v3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rdkit.Chem as Chem\n",
    "from numpy.random import default_rng\n",
    "import torch\n",
    "from ast import literal_eval\n",
    "from torch import nn, optim\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from tdc import Oracle\n",
    "import subprocess\n",
    "\n",
    "from utils import fingerprints_from_mol\n",
    "from scripts.write_config_bradley_terry import write_REINVENT_config_bradley_terry\n",
    "from models.RandomForest import RandomForestReg, RandomForestClf\n",
    "from scripts.acquisition import select_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we need to install the custom reinvent scoring package to support the Bradley-Terry model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.6 | packaged by conda-forge | (default, Jun  1 2020, 18:57:50) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "# print python version\n",
    "import sys\n",
    "\n",
    "# Print Python version\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: reinvent-scoring\n",
      "Version: 0.0.73\n",
      "Summary: Scoring functions for Reinvent\n",
      "Home-page: https://github.com/MolecularAI/reinvent-scoring.git\n",
      "Author: MolecularAI\n",
      "Author-email: patronov@gmail.com\n",
      "License: UNKNOWN\n",
      "Location: /home/springnuance/reinvent-hitl/reinvent-scoring\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show reinvent_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install scikit-learn=0.21.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If there already exists reinvent_scoring, we should uninstall it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: reinvent-scoring 0.0.73\n",
      "Uninstalling reinvent-scoring-0.0.73:\n",
      "  Successfully uninstalled reinvent-scoring-0.0.73\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall -y reinvent_scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we install the custom reinvent scoring package\n",
    "##### The flag -e means that the package is installed in editable mode, so that changes to the code will be immediately available without reinstalling the package. All package info is stored in the setup.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/springnuance/reinvent-hitl/reinvent-scoring\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: reinvent-scoring\n",
      "  Running setup.py develop for reinvent-scoring\n",
      "Successfully installed reinvent-scoring-0.0.73\n",
      "Obtaining file:///home/springnuance/reinvent-hitl/reinvent-chemistry\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: reinvent-chemistry\n",
      "  Attempting uninstall: reinvent-chemistry\n",
      "    Found existing installation: reinvent-chemistry 0.0.51\n",
      "    Uninstalling reinvent-chemistry-0.0.51:\n",
      "      Successfully uninstalled reinvent-chemistry-0.0.51\n",
      "  Running setup.py develop for reinvent-chemistry\n",
      "Successfully installed reinvent-chemistry-0.0.51\n",
      "Obtaining file:///home/springnuance/reinvent-hitl/reinvent-models\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: reinvent-models\n",
      "  Attempting uninstall: reinvent-models\n",
      "    Found existing installation: reinvent-models 0.0.15rc1\n",
      "    Uninstalling reinvent-models-0.0.15rc1:\n",
      "      Successfully uninstalled reinvent-models-0.0.15rc1\n",
      "  Running setup.py develop for reinvent-models\n",
      "Successfully installed reinvent-models-0.0.15rc1\n"
     ]
    }
   ],
   "source": [
    "! pip install -e \"/home/springnuance/reinvent-hitl/reinvent-scoring\"\n",
    "! pip install -e \"/home/springnuance/reinvent-hitl/reinvent-chemistry\"\n",
    "! pip install -e \"/home/springnuance/reinvent-hitl/reinvent-models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install -y scikit-learn=0.21.3\n",
    "! pip list | grep reinvent_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ECFP_dataset(init_train_set_path, num_train_samples):\n",
    "    \"\"\"\n",
    "        Load background training data used to pre-train the predictive model    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading D0\")\n",
    "    train_set = pd.read_csv(init_train_set_path)\n",
    "    feature_cols = [f\"bit{i}\" for i in range(2048)]\n",
    "    target_col = [\"activity\"]\n",
    "    smiles_train = train_set[\"smiles\"].values.reshape(-1)\n",
    "    x_train = train_set[feature_cols].values\n",
    "    y_train = train_set[target_col].values.reshape(-1)\n",
    "    sample_weight = np.array([1. for i in range(len(x_train))])\n",
    "    print(\"The feature matrix shape: \", x_train.shape)\n",
    "    print(\"The labels shape: \", y_train.shape)\n",
    "\n",
    "    train_sample = train_set[train_set[\"activity\"] == 1].sample(num_train_samples).smiles.tolist()\n",
    "    return x_train, y_train, sample_weight, smiles_train, train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_Bradley_Terry_model.bradley_terry import BradleyTerryModel\n",
    "\n",
    "def check_create(path):\n",
    "    \"\"\"\n",
    "    Check if the directory exists, if not, create it.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "def run_HITL_classify(\n",
    "        seed, dirname, reinvent_dir, reinvent_env, output_dir, restart,\n",
    "        feedback_type, # score, compare, rank\n",
    "        model_pretrained_path, # Path to the pretrained model before HITL_round_1\n",
    "        model_pretrained_name,\n",
    "        num_rounds, # number of rounds, corresponding to R in the paper\n",
    "        num_iters, # number of molecules shown at each iteration to the human for feedback, corresponding to T in the paper\n",
    "        num_queries, # number of molecules shown to the simulated chemist at each iteration\n",
    "        REINVENT_n_steps, # number of REINVENT optimization steps\n",
    "        acquisition, # acquisition: 'uncertainty', 'random', 'thompson', 'greedy' (if None run with no human interaction)\n",
    "        sigma_noise, # noise level for simulated chemist's responses\n",
    "        threshold # threshold for high scoring molecules\n",
    "        ):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    rng = default_rng(seed)\n",
    "    \n",
    "    ################################################\n",
    "    # DEFINING REINVENT JOBNAME, JOBID, OUTPUT_DIR #\n",
    "    ################################################\n",
    "\n",
    "    jobname = \"fine-tune predictive component HITL\"\n",
    "    jobid = f\"{dirname}_rounds_{num_rounds}_iters_{num_iters}_queries_{num_queries}_{acquisition}_noise_{sigma_noise}\"\n",
    "    conf_filename = \"config.json\"\n",
    "\n",
    "    # create root output dir\n",
    "    check_create(output_dir)\n",
    "\n",
    "    # create HITL round folders to store results\n",
    "    for HITL_round in np.arange(1, num_rounds + 1):\n",
    "        check_create(f\"{output_dir}/HITL_round_{HITL_round}\")\n",
    "    \n",
    "    # Copy the pretrained model to the first iteration\n",
    "    shutil.copy2(f\"{model_pretrained_path}\", f\"{output_dir}/HITL_round_1\")\n",
    "    \n",
    "    # store expert scores\n",
    "    expert_score = []\n",
    "\n",
    "    # multi-parameter optimization (MPO) loop\n",
    "    print(f\"Running MPO experiment with rounds {num_rounds}, iters {num_iters}, queries {num_queries}, seed {seed}. \\n Results will be saved at {output_dir}\")\n",
    "\n",
    "\n",
    "    for HITL_round in range(1, num_rounds + 1):\n",
    "\n",
    "        print(\"=====================================\")\n",
    "        print(f\"HITL round = {HITL_round}\")\n",
    "\n",
    "        HITL_round_output_dir = f\"{output_dir}/HITL_round_{HITL_round}\"\n",
    "        \n",
    "        if feedback_type == \"scoring\":\n",
    "            raise NotImplementedError(\"Score feedback not implemented yet.\")\n",
    "        elif feedback_type == \"comparing\":\n",
    "            configuration_JSON_path =\\\n",
    "                write_REINVENT_config_bradley_terry(reinvent_dir, jobid, jobname,\n",
    "                                                    HITL_round_output_dir, conf_filename\n",
    "                                                    )\n",
    "        elif feedback_type == \"ranking\":\n",
    "            raise NotImplementedError(\"Rank feedback not implemented yet.\")\n",
    "        \n",
    "        print(f\"Creating config file: {configuration_JSON_path}.\")\n",
    "\n",
    "        configuration = json.load(open(os.path.join(HITL_round_output_dir, conf_filename)))\n",
    "\n",
    "        # write specified number of RL optimization steps in configuration\n",
    "        # (example: if num_rounds = 5 (rounds) and Reinvent REINVENT_n_steps = 100, we will do 5*100 RL optimization steps)\n",
    "        configuration[\"parameters\"][\"reinforcement_learning\"][\"n_steps\"] = REINVENT_n_steps\n",
    "\n",
    "        # write the model path at current HITL round\n",
    "        configuration_scoring_function = configuration[\"parameters\"][\"scoring_function\"][\"parameters\"]\n",
    "        \n",
    "        configuration_scoring_function[0][\"specific_parameters\"][\"model_pretrained_path\"] =\\\n",
    "            f\"{HITL_round_output_dir}/{model_pretrained_name}\"\n",
    "        \n",
    "        configuration[\"parameters\"][\"scoring_function\"][\"parameters\"] = configuration_scoring_function\n",
    "        \n",
    "        # write the updated configuration file to the disc\n",
    "        configuration_JSON_path = f\"{os.getcwd()}/{HITL_round_output_dir}/{conf_filename}\"\n",
    "        print(\"The configuration file path: \", configuration_JSON_path)\n",
    "\n",
    "        with open(configuration_JSON_path, 'w') as f:\n",
    "            json.dump(configuration, f, indent=4, sort_keys=True)\n",
    "        \n",
    "        # Initialize human feedback model\n",
    "\n",
    "        print(f\"Loading {feedback_type} feedback model.\")\n",
    "\n",
    "        if feedback_type == \"scoring\":\n",
    "            raise NotImplementedError(\"Score feedback not implemented yet.\")\n",
    "        elif feedback_type == \"comparing\":\n",
    "            feedback_model = BradleyTerryModel()\n",
    "            # Load the state dict\n",
    "            feedback_model.load_state_dict(torch.load(f\"{HITL_round_output_dir}/{model_pretrained_name}\"))\n",
    "            print(\"Loading Bradley Terry model successfully\")\n",
    "        elif feedback_type == \"ranking\":\n",
    "            raise NotImplementedError(\"Rank feedback not implemented yet.\")\n",
    "\n",
    "        # generate a pool of unlabelled compounds with REINVENT\n",
    "        print(\"Run REINVENT\")                \n",
    "        command = f\"{reinvent_env}/bin/python\"\n",
    "        script = f\"{reinvent_dir}/input.py\"\n",
    "        config_path = configuration_JSON_path\n",
    "        stderr_file = f\"{HITL_round_output_dir}/run.err\"\n",
    "        stdout_file = f\"{HITL_round_output_dir}/run.out\"\n",
    "\n",
    "        # Construct the full command to run\n",
    "        cmd = [command, script, config_path]\n",
    "        # Open the file to which you want to redirect stderr and stdout\n",
    "        with open(stderr_file, 'w') as ferr, open(stdout_file, 'w') as fout:\n",
    "            # Execute the command\n",
    "            result = subprocess.run(cmd, text=True, stdout=fout, stderr=ferr)\n",
    "\n",
    "        # Check the result\n",
    "        print(\"Exit code:\", result.returncode)\n",
    "        \n",
    "        #############################################################################\n",
    "        # REINVENT HAS OUTPUT THE RESULT in path f\"{HITL_round_output_dir}/results\" #\n",
    "        #############################################################################\n",
    "\n",
    "        # Now we open the csv file\n",
    "        with open(f\"{HITL_round_output_dir}/results/scaffold_memory.csv\", 'r') as file:\n",
    "            data = pd.read_csv(file)\n",
    "\n",
    "        data_len = len(data)\n",
    "        colnames = list(data) \n",
    "        smiles = data['SMILES']\n",
    "        bioactivity_score = data['bioactivity'] # the same as raw_bioactivity since no transformation applied\n",
    "        raw_bioactivity_score = data['raw_bioactivity']\n",
    "        high_scoring_threshold = threshold\n",
    "        # save the indexes of high scoring molecules for bioactivity\n",
    "        high_scoring_idx = bioactivity_score > high_scoring_threshold\n",
    "\n",
    "        # Scoring component values\n",
    "        scoring_component_names = [s.split(\"raw_\")[1] for s in colnames if \"raw_\" in s]\n",
    "        print(f\"scoring components: {scoring_component_names}\")\n",
    "        x = np.array(data[scoring_component_names])\n",
    "        print(f'Scoring component matrix dimensions: {x.shape}')\n",
    "        x = x[high_scoring_idx,:]\n",
    "\n",
    "        # Only analyse highest scoring molecules\n",
    "        smiles = smiles[high_scoring_idx]\n",
    "        bioactivity_score = bioactivity_score[high_scoring_idx]\n",
    "        raw_bioactivity_score = raw_bioactivity_score[high_scoring_idx]\n",
    "        print(f'{len(smiles)} high-scoring (> {high_scoring_threshold}) molecules')\n",
    "\n",
    "        if len(smiles) == 0:\n",
    "            smiles = data['SMILES']\n",
    "            print(f'{len(smiles)} molecules')\n",
    "\n",
    "               \n",
    "        # store molecule indexes selected for feedback\n",
    "        selected_feedback = np.empty(0).astype(int)\n",
    "        human_sample_weight = np.empty(0).astype(float)\n",
    "        # store number of accepted queries (y = 1) at each iteration\n",
    "        n_accept = []\n",
    "\n",
    "        ########################### HITL rounds ######################################\n",
    "        \n",
    "        for iteration in np.arange(num_iters): # T number of HITL iterations\n",
    "            print(\"=====================================\")\n",
    "            print(f\"Round = {REINVENT_round}, Iteration = {iteration}\")\n",
    "            \n",
    "            # classify \n",
    "            model = RandomForestClf(fitted_model)\n",
    "            \n",
    "            if len(smiles) > num_queries:\n",
    "                new_query = select_query(data, num_queries, list(smiles), model, selected_feedback, acquisition, rng) # select n smiles with Active Learning\n",
    "            else:\n",
    "                new_query = select_query(data, len(smiles), list(smiles), model, selected_feedback, acquisition, rng)\n",
    "            \n",
    "            # Initialize the expert values vector\n",
    "            s_bioactivity = [] # for scores (between 0 and 1)\n",
    "            v_bioactivity = [] # for continuous feedback (regression)\n",
    "            \n",
    "            # Get expert feedback on selected queries\n",
    "            print(new_query)\n",
    "            for i in new_query:\n",
    "                cur_mol = data.iloc[i][\"SMILES\"]\n",
    "                print(cur_mol)\n",
    "                value = feedback_model.human_score(cur_mol, sigma_noise)\n",
    "                s_bioactivity.append(value)\n",
    "            \n",
    "            # Get raw scores and transformed score (if any) from the high scoring molecules in U\n",
    "            raw_scoring_component_names = [\"raw_\"+name for name in scoring_component_names] \n",
    "            x_raw = data[raw_scoring_component_names].to_numpy()\n",
    "            x =  data[scoring_component_names].to_numpy()\n",
    "\n",
    "            # get (binary) simulated chemist's responses\n",
    "            \n",
    "            new_y = np.array([1 if s > 0.5 else 0 for s in s_bioactivity])\n",
    "            accepted = new_y.tolist()\n",
    "            \n",
    "            expert_score += [accepted]\n",
    "            n_accept += [sum(accepted)]\n",
    "\n",
    "            print(f\"Feedback idx at iteration {REINVENT_round}, {iteration}: {new_query}\")\n",
    "            print(f\"Number of accepted molecules at iteration {REINVENT_round}, {iteration}: {n_accept[iteration]}\")   \n",
    "            \n",
    "            # append feedback\n",
    "            if len(new_y) > 0:\n",
    "                selected_feedback = np.hstack((selected_feedback, new_query))\n",
    "\n",
    "            mask = np.ones(N, dtype=bool)\n",
    "            mask[selected_feedback] = False\n",
    "\n",
    "            # use the augmented training data to retrain the model\n",
    "            new_smiles = data.iloc[new_query].SMILES.tolist()\n",
    "            new_mols = [Chem.MolFromSmiles(s) for s in new_smiles]\n",
    "            new_x = fingerprints_from_mol(new_mols, type = \"counts\")\n",
    "            new_human_sample_weight = np.array([s if s > 0.5 else 1-s for s in s_bioactivity])\n",
    "            sample_weight = np.concatenate([sample_weight, new_human_sample_weight])\n",
    "            print(len(new_x), len(new_y))\n",
    "            x_train = np.concatenate([x_train, new_x])\n",
    "            y_train = np.concatenate([y_train, new_y])\n",
    "            smiles_train = np.concatenate([smiles_train, new_smiles])\n",
    "            print(f\"Augmented train set size at iteration {REINVENT_round}: {x_train.shape[0]} {y_train.shape[0]}\")\n",
    "            # save augmented training data\n",
    "            D_r = pd.DataFrame(np.concatenate([smiles_train.reshape(-1,1), x_train, y_train.reshape(-1,1)], 1))\n",
    "            D_r.columns = [\"SMILES\"] + [f\"bit{i}\" for i in range(x_train.shape[1])] + [\"target\"]\n",
    "            D_r.to_csv(os.path.join(output_dir, f\"augmented_train_set_iter{REINVENT_round}.csv\"))\n",
    "\n",
    "            # re-fit and save the model using the augmented train set and save to new directory\n",
    "            model_new_savefile = output_dir + '/{}_iteration_{}.pkl'.format(predictive_model_name, REINVENT_round)\n",
    "            model._retrain(x_train, y_train, sample_weight = sample_weight, save_to_path = model_new_savefile)\n",
    "            fitted_model = pickle.load(open(model_new_savefile, 'rb'))\n",
    "\n",
    "            # get current configuration\n",
    "            configuration = json.load(open(os.path.join(output_dir, conf_filename)))\n",
    "            conf_filename = \"iteration{}_config.json\".format(REINVENT_round)    \n",
    "\n",
    "            # modify model path in configuration\n",
    "            configuration_scoring_function = configuration[\"parameters\"][\"scoring_function\"][\"parameters\"]\n",
    "            for i in range(len(configuration_scoring_function)):\n",
    "                if configuration_scoring_function[i][\"component_type\"] == \"predictive_property\":\n",
    "                    configuration_scoring_function[i][\"specific_parameters\"][\"model_path\"] = model_new_savefile\n",
    "\n",
    "            # Keep agent checkpoint\n",
    "            if REINVENT_round == 1:\n",
    "                configuration[\"parameters\"][\"reinforcement_learning\"][\"agent\"] = os.path.join(initial_dir, \"results/Agent.ckpt\")\n",
    "            else:\n",
    "                configuration[\"parameters\"][\"reinforcement_learning\"][\"agent\"] = os.path.join(output_dir, \"results/Agent.ckpt\")\n",
    "\n",
    "        root_output_dir = os.path.expanduser(\"{}_seed{}\".format(jobid, seed))\n",
    "\n",
    "        # Define new directory for the next round\n",
    "        output_dir = os.path.join(root_output_dir, \"iteration{}_{}\".format(HITL_round, acquisition))\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        print(output_dir)\n",
    "\n",
    "        # modify log and result paths in configuration\n",
    "        configuration[\"logging\"][\"logging_path\"] = os.path.join(output_dir, \"progress.log\")\n",
    "        configuration[\"logging\"][\"result_folder\"] = os.path.join(output_dir, \"results\")\n",
    "\n",
    "        # write the updated configuration file to the disc\n",
    "        configuration_JSON_path = os.path.join(output_dir, conf_filename)\n",
    "        with open(configuration_JSON_path, 'w') as f:\n",
    "            json.dump(configuration, f, indent=4, sort_keys=True)\n",
    "\n",
    "    r = np.arange(len(expert_score))\n",
    "    m_score = [np.mean(expert_score[i]) for i in r]\n",
    "    print(\"Mean expert score : \", m_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/springnuance/reinvent-hitl/Base-Code-Binh\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/springnuance/reinvent-hitl/reinvent-scoring\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: reinvent-scoring\n",
      "  Attempting uninstall: reinvent-scoring\n",
      "    Found existing installation: reinvent-scoring 0.0.73\n",
      "    Uninstalling reinvent-scoring-0.0.73:\n",
      "      Successfully uninstalled reinvent-scoring-0.0.73\n",
      "  Running setup.py develop for reinvent-scoring\n",
      "Successfully installed reinvent-scoring-0.0.73\n",
      "Running MPO experiment with rounds 2, iters 10, queries 10, seed 42. \n",
      " Results will be saved at output_feedback_comparing\n",
      "=====================================\n",
      "HITL round = 1\n",
      "Creating config file: output_feedback_comparing/HITL_round_1/config.json.\n",
      "The configuration file path:  /home/springnuance/reinvent-hitl/Base-Code-Binh/output_feedback_comparing/HITL_round_1/config.json\n",
      "Loading comparing feedback model.\n",
      "Loading Bradley Terry model successfully\n",
      "Run REINVENT\n",
      "Exit code: 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output_feedback_comparing/HITL_round_1/scaffold_memory.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9ed98dddc1c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0macquisition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# acquisition: 'uncertainty', 'random', 'thompson', 'greedy' (if None run with no human interaction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0msigma_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# noise level for simulated chemist's responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mthreshold\u001b[0m \u001b[0;31m# threshold for high scoring molecules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-18-4281846f2fb0>\u001b[0m in \u001b[0;36mrun_HITL_classify\u001b[0;34m(seed, dirname, reinvent_dir, reinvent_env, output_dir, restart, feedback_type, model_pretrained_path, model_pretrained_name, num_rounds, num_iters, num_queries, REINVENT_n_steps, acquisition, sigma_noise, threshold)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Now we open the csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{HITL_round_output_dir}/scaffold_memory.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output_feedback_comparing/HITL_round_1/scaffold_memory.csv'"
     ]
    }
   ],
   "source": [
    "! pip install -e \"/home/springnuance/reinvent-hitl/reinvent-scoring\"\n",
    "\n",
    "seed = 42\n",
    "dirname = \"outputs\"\n",
    "restart = False # If restart is True, we would rerun everything\n",
    "                # If restart is False, we would continue from the latest found iteration\n",
    "# change these path variables as required\n",
    "reinvent_dir = os.path.expanduser(\"/home/springnuance/reinvent-hitl/Reinvent\") # We must use absolute path\n",
    "reinvent_env = os.path.expanduser(\"/home/springnuance/miniconda3/envs/ReinventCommunity\") # We must use absolute path\n",
    "\n",
    "# the performance of the initial model should not be good. Specifically, it should work at 0.5 accuracy \n",
    "# If the model is too good, retrain the model to become weaker, we are trying to make the model to learn via HITL\n",
    "\n",
    "feedback_type = \"comparing\" # scoring, comparing, ranking\n",
    "output_dir = f\"output_feedback_{feedback_type}\"\n",
    "\n",
    "# feedback type as scoring:\n",
    "# Given a molecule, what is the probability that the molecule is active regarding DRD2?  \n",
    "# init_model_path = \"/home/springnuance/reinvent-hitl/Base-Code-Binh/training_Random_Forest_model/random_forest_model.pkl\"\n",
    "\n",
    "# feedback type as comparing:\n",
    "# Given two molecules, what is the probability that the first molecule is more active than the second molecule regarding DRD2?\n",
    "\n",
    "model_pretrained_path = \"/home/springnuance/reinvent-hitl/Base-Code-Binh/training_Bradley_Terry_model/bradley_terry_model.pth\"\n",
    "model_pretrained_name = \"bradley_terry_model.pth\"\n",
    "# feedback type as ranking:\n",
    "# Given N molecules, what are the orders of preference of these molecules regarding DRD2?\n",
    "# init_model_path = \"/home/springnuance/reinvent-hitl/Base-Code-Binh/training_List_Net_model/list_net_model.pth\"\n",
    "\n",
    "num_rounds = 2 # number of rounds\n",
    "REINVENT_n_steps = 1 # number of REINVENT optimization steps\n",
    "\n",
    "# Please look at the thompson sampling code and fix it!\n",
    "acquisition = \"thompson\" # acquisition: 'uncertainty', 'random', 'thompson', 'greedy' (if None run with no human interaction)\n",
    "\n",
    "sigma_noise = 0.0 # noise level for simulated chemist's responses\n",
    "num_iters = 10 # number of molecules shown at each iteration to the human for feedback\n",
    "num_queries = 10 # number of molecules shown to the simulated chemist at each iteration (10 pairs)\n",
    "threshold = 0.5 # threshold for high scoring molecules\n",
    "    \n",
    "run_HITL_classify(\n",
    "        seed, dirname, reinvent_dir, reinvent_env, output_dir, restart,\n",
    "        feedback_type, # score, compare, rank\n",
    "        model_pretrained_path, \n",
    "        model_pretrained_name,\n",
    "        num_rounds, # number of rounds, corresponding to R in the paper\n",
    "        num_iters, # number of molecules shown at each iteration to the human for feedback, corresponding to T in the paper\n",
    "        num_queries, # number of molecules shown to the simulated chemist at each iteration\n",
    "        REINVENT_n_steps, # number of REINVENT optimization steps\n",
    "        acquisition, # acquisition: 'uncertainty', 'random', 'thompson', 'greedy' (if None run with no human interaction)\n",
    "        sigma_noise, # noise level for simulated chemist's responses\n",
    "        threshold # threshold for high scoring molecules\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc_env_hitl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
