{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from tdc import Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0005901676463998309, 0.001288978921112421, 0.00023099921452992108, 0.0014480634352253118, 0.00023229149413919086, 0.007226910417238079, 0.03177009486102104, 0.00018465196104625156, 0.0006181948403864203, 0.017767959705749174, 0.0001353658415040822, 7.584214925439865e-05, 0.00011932658307753439, 0.00019596215439119096, 5.0310012366721855e-05, 0.00153997550154151, 0.004863899555067754, 0.0013627517075909072, 0.00022636359589811632, 0.9999907104594737, 0.002364173240172854, 0.9999987724917501, 0.0004128064910287372, 4.515114620151249e-05, 0.9999899941967955, 3.439700377056164e-05, 0.005426522090583173, 0.00015732934690030053, 0.0003269993975438858, 0.0003363411034820678, 0.0055611952891646, 0.9890921212669289, 0.9999988552735308, 0.9905578204031097, 0.00045109684719108454, 0.0012732716327158269, 0.0005288131087850552, 0.0021161386976471714, 0.014029352175312171, 0.013244877011903917, 0.9999973055784004, 0.9999998974481366, 0.0032787785378776348, 0.0002981816185162872, 0.00042036779434050873, 0.001814834216310095, 0.0011774454500123486, 0.0010582825849395822, 0.00018476092638482834, 0.0007292199909660255, 0.0003754157463252528, 0.0041114146581634455, 0.9999979172189909, 0.006261474972710569, 0.9999979450327827, 0.0001879892487192794, 0.003896816782062064, 8.958536744060414e-05, 0.0004092054668379563, 0.004782212562425562, 0.002209865026652471, 0.9999945012677769, 0.0004092054668379563, 0.0002865270730883282, 0.0013202989172575188, 0.001810292038855387, 0.0006702484938684605, 0.00019586360854653503, 0.000383641159481653, 0.9999975683231251, 0.9968642932401842, 0.9935260639669249, 0.0006438897722014135, 0.002404273805165934, 0.9999994910723929, 0.001938311493176282, 0.002065353515301654, 6.979447770930128e-05, 0.999999540363816, 0.0005154213061424374, 0.9999973055784004, 0.99999481531433, 0.0012062095273522617, 0.0003016502942466645, 0.9999973055784004, 0.9999993478737205, 0.014115803656290614, 0.00033702279693329044, 0.00042505771109273055, 0.0014580654985439539, 0.000847669237280818, 0.0002342716665780194, 0.9999875912605337, 0.9422954647225189, 0.0020147324375968233, 0.9999979549672328, 0.0006876249604984917, 0.04545445072064346, 0.9999979508542166, 0.0004685846458284135, 0.003358949275224765, 0.9918497484692325, 0.000740826918224148, 0.0005622464610039502, 0.0010582825849395822, 0.984760736217924, 0.0003237994420893527, 0.0003587470337912561, 0.9999998969657062, 0.000177739522203383, 0.00034957783687254853, 0.9999999639608361, 0.00015421563601894172, 0.00025857824397611283, 0.0022870322544063623, 0.0005530418948680203, 0.000345994363763932, 0.0005663269162095706, 0.9999981263100579, 0.0005810591261236133, 0.001476121604255974, 0.9999889064723066, 0.00018579878833621075, 0.001191370188987206, 0.0010978339976112244, 0.9999967152818698, 0.0012575329606122417, 0.9999989916680257, 0.00048384335155571734, 9.738623776762325e-05, 8.876027118262232e-05, 0.9999979460964545, 0.00021676418371091347, 0.0013592987179396636, 0.9999998969657062, 0.0027752367783352608, 0.9688444477416027, 0.004040068207550039, 0.9999979528706872, 0.0003673147106466183, 0.0007585474459448414, 0.0046734567718272, 0.0013648549463819283, 0.989853043086398, 0.006187637068708067, 0.9999936356426508, 0.0008113241314145356, 0.00042064769864728327, 0.0007064421619055175, 0.9999990380336338, 0.0005559191528330199, 0.00015477336953094857, 0.0017319101087791414, 0.9970724635338566, 0.0017120153193430651, 0.0003133717004575522, 0.9972006958914625, 0.001347681150180494, 0.9999988707993551, 0.9474463159595384, 0.001936425678589996, 9.681679633224503e-05, 0.9999981263100579, 0.0019028051472603925, 0.9999991870643247, 0.0009439804100662562, 1.7974291153285605e-05, 0.99999481531433, 0.9999993817505263, 0.9999979549672328, 0.002514533367109411, 0.005300985628557938, 0.997080057938897, 0.000421870379538592, 0.0008055625738994726, 9.682858953475047e-05, 0.0026566603742818867, 0.9999888061243491, 0.9999979420192974, 0.00045934865975602666, 0.0001257025618620002, 0.9999917069555132, 0.002807497162824487, 0.0005092048368857076, 6.857075109763618e-05, 4.66497221930094e-05, 0.0013342240503595828, 0.0002897644899935572, 0.00021951242930383935, 0.003614265706607552, 0.003236775616799799, 0.0002585625975243525, 1.8074112189849902e-05, 0.0004359816677336856, 7.584214925439865e-05, 0.9999989798534827, 0.0005658792264689748, 0.996677788804331, 4.8878943261661794e-05, 0.0003719319401692122, 0.003078488554464032, 0.9561598241613558, 0.9999990324768554, 0.9829503430432021, 0.00036461921911503585, 2.2147402930533356e-05, 0.0034735169030902157, 0.00024971500053392616, 0.00037815784102775536, 0.9999979485955824, 0.9999997372338433, 0.0004819792511980272, 0.99999751014689, 0.0008108943006821922, 0.005483673009565892, 9.704775701398966e-05, 0.0017957755032875175, 0.9999984926091092, 0.0023386270272292534, 0.9968642932401842, 0.0008258382853335917, 0.00095549861271999, 0.004174886952397837, 0.0005315070657719729, 0.9999993478737205, 0.0005432504687833247, 0.00013488827625616882, 0.00384358999185699, 0.9999998916661036, 0.0006082499504658582, 0.00035381608185397804, 0.0036591717264559715, 0.0008569518530564626, 0.0023707204088490416, 0.00018925489290841238, 0.9999997261291642, 0.9999979549672328, 0.9999981263100579, 0.9999999202103159, 0.001788454414272621]\n"
     ]
    }
   ],
   "source": [
    "# Load data and Oracle\n",
    "oracle = Oracle(name='DRD2')\n",
    "data = pd.read_csv(\"small_drd2_data.csv\")\n",
    "smiles_list = data['smiles'].tolist()\n",
    "proba_molecules = [oracle(smiles) for smiles in smiles_list]\n",
    "print(proba_molecules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing morgan fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240, 2048])\n"
     ]
    }
   ],
   "source": [
    "def compute_fingerprints(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
    "    else:\n",
    "        return np.zeros((2048,), dtype=int)\n",
    "\n",
    "features = torch.tensor([compute_fingerprints(smiles) for smiles in smiles_list])\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate pairs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs_labels(features, proba_molecules, smiles_list, num_sets = 100):\n",
    "    n = len(features)\n",
    "    features_1 = []\n",
    "    features_2 = []\n",
    "    labels_proba = []\n",
    "    labels_binary = []\n",
    "    smiles_1 = []\n",
    "    smiles_2 = []\n",
    "\n",
    "    # set seed\n",
    "    np.random.seed(42) \n",
    "    \n",
    "    count = 0\n",
    "    # sample unrepeated num_sets number of pairs\n",
    "    while count < num_sets:\n",
    "        i = np.random.randint(0, n)\n",
    "        j = np.random.randint(0, n)\n",
    "        if i != j:\n",
    "            features_1.append(features[i])\n",
    "            features_2.append(features[j])\n",
    "            smiles_1.append(smiles_list[i])\n",
    "            smiles_2.append(smiles_list[j])\n",
    "            # convert to float\n",
    "            proba_better = torch.tensor(proba_molecules[i] - proba_molecules[j], dtype=torch.float32)\n",
    "            proba_smiles1_better_than_smiles2 = torch.sigmoid(proba_better)\n",
    "            labels_proba.append(proba_smiles1_better_than_smiles2)\n",
    "            labels_binary.append(1 if proba_molecules[i] > proba_molecules[j] else 0)\n",
    "            count += 1\n",
    "        \n",
    "    # Convert to numpy\n",
    "    labels_proba = np.array(labels_proba)\n",
    "    labels_binary = np.array(labels_binary)\n",
    "    features_1 = torch.stack(features_1)\n",
    "    features_2 = torch.stack(features_2)\n",
    "\n",
    "    return features_1, features_2, labels_proba, labels_binary, smiles_1, smiles_2\n",
    "\n",
    "features_1, features_2, labels_proba, labels_binary, smiles_1, smiles_2 =\\\n",
    "    generate_pairs_labels(features, proba_molecules, smiles_list, num_sets=len(smiles_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240, 2048])\n",
      "torch.Size([240, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(features_1.shape)\n",
    "print(features_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Why is test_size very high here?\n",
    "# This is because we are using active learning later\n",
    "# Where the Human in the loop process would generate more better training data for the model\n",
    "# So the amount of training data to train the initial model is not important, as the model should not be better than random guess\n",
    "\n",
    "\n",
    "features_1_train, features_1_test,\\\n",
    "    features_2_train, features_2_test,\\\n",
    "    labels_proba_train, labels_proba_test,\\\n",
    "    labels_binary_train, labels_binary_test,\\\n",
    "    smiles_1_train, smiles_1_test,\\\n",
    "    smiles_2_train, smiles_2_test = train_test_split(features_1, features_2, labels_proba, labels_binary, smiles_1, smiles_2, test_size=0.9, random_state=42)\n",
    "\n",
    "# Now we need to save them \n",
    "\n",
    "small_drd2_training_data = pd.DataFrame()\n",
    "small_drd2_training_data['smiles_1'] = smiles_1_train\n",
    "small_drd2_training_data['smiles_2'] = smiles_2_train\n",
    "small_drd2_training_data['label_proba'] = labels_proba_train\n",
    "small_drd2_training_data['label_binary'] = labels_binary_train\n",
    "\n",
    "small_drd2_training_data.to_csv(\"small_drd2_training_data.csv\", index=False)\n",
    "\n",
    "small_drd2_testing_data = pd.DataFrame()\n",
    "small_drd2_testing_data['smiles_1'] = smiles_1_test\n",
    "small_drd2_testing_data['smiles_2'] = smiles_2_test\n",
    "small_drd2_testing_data['label_proba'] = labels_proba_test\n",
    "small_drd2_testing_data['label_binary'] = labels_binary_test\n",
    "\n",
    "small_drd2_testing_data.to_csv(\"small_drd2_testing_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.002882737914721171\n",
      "Model trained and saved\n"
     ]
    }
   ],
   "source": [
    "from bradley_terry import BradleyTerryModel\n",
    "    \n",
    "# Training the model\n",
    "model = BradleyTerryModel(feature_dim=2048)\n",
    "\n",
    "# When using Binary Cross-Entropy Loss (BCELoss) in neural networks, the input expected by the \n",
    "# loss function is a list of probabilities, not binary values (0 or 1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create torch data loader\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Assuming X_train and y_train are numpy arrays, convert them to PyTorch tensors\n",
    "features_1_train_tensor = torch.tensor(features_1_train).float()  \n",
    "features_2_train_tensor = torch.tensor(features_2_train).float()\n",
    "labels_proba_train_tensor = torch.tensor(labels_proba_train).float()\n",
    "\n",
    "features_1_test_tensor = torch.tensor(features_1_test).float()\n",
    "features_2_test_tensor = torch.tensor(features_2_test).float()\n",
    "labels_proba_test_tensor = torch.tensor(labels_proba_test).float()\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(features_1_train_tensor, features_2_train_tensor, labels_proba_train_tensor)\n",
    "test_dataset = TensorDataset(features_1_test_tensor, features_2_test_tensor, labels_proba_test_tensor)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 64  # You can adjust the batch size as needed\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def train(model, train_loader, epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for features_1, features_2, labels_proba in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features_1, features_2)\n",
    "            loss = criterion(output, labels_proba.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(features)}')\n",
    "\n",
    "train(model, train_loader)\n",
    "\n",
    "# save state dict\n",
    "torch.save(model.state_dict(), \"bradley_terry_model.pth\")\n",
    "\n",
    "print(\"Model trained and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform prediction on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "def compute_metrics(labels, predictions):\n",
    "    \"\"\"\n",
    "    Compute classification metrics: accuracy, precision, recall, F1 score, and MCC.\n",
    "    \n",
    "    Args:\n",
    "    labels (list[int]): True binary labels.\n",
    "    predictions (list[int]): Predicted binary labels.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the computed metrics.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'F1': f1,\n",
    "        'MCC': mcc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7916666666666666\n",
      "Precision: 0.8173913043478261\n",
      "Recall: 0.7966101694915254\n",
      "F1: 0.8068669527896997\n",
      "MCC: 0.5810445921137357\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = BradleyTerryModel(feature_dim=2048)\n",
    "\n",
    "# Load the state dict\n",
    "model.load_state_dict(torch.load(\"bradley_terry_model.pth\"))\n",
    "\n",
    "# Finally we perform prediction\n",
    "model.eval()\n",
    "\n",
    "prediction_binary_test = []\n",
    "with torch.no_grad():\n",
    "    for features_1, features_2, labels_proba in test_loader:\n",
    "        outputs = model(features_1, features_2)\n",
    "        prediction_binary_test.extend(outputs.squeeze().tolist())\n",
    "\n",
    "prediction_binary_test = (torch.tensor(prediction_binary_test) > 0.5).int().tolist()\n",
    "\n",
    "metrics = compute_metrics(labels_binary_test, prediction_binary_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics['accuracy'])\n",
    "print(\"Precision:\", metrics['precision'])\n",
    "print(\"Recall:\", metrics['recall'])\n",
    "print(\"F1:\", metrics['F1'])\n",
    "print(\"MCC:\", metrics['MCC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model performs very weakly, which means that we can now use this model as a human component for HITL. Ideally, the initial model should be no better than random guess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
