{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import Crippen\n",
    "from rdkit import DataStructs\n",
    "from numpy.random import default_rng\n",
    "import torch\n",
    "from ast import literal_eval\n",
    "from torch import nn, optim\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from tdc import Oracle\n",
    "import subprocess\n",
    "\n",
    "from rdkit.Chem import RDConfig\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
    "# now you can import sascore!\n",
    "import sascorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/springnuance/reinvent-hitl/Base-Code-Binh\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_Bradley_Terry_model.bradley_terry import BradleyTerryModel\n",
    "from training_Rank_ListNet_model.rank_listnet import RankListNetModel\n",
    "from training_Score_Regression_model.score_regression import ScoreRegressionModel\n",
    "from scripts.helper import load_drd2_dataset, write_REINVENT_config, change_config_json, \\\n",
    "                    read_scaffold_result, load_feedback_model, smiles_human_score, \\\n",
    "                    compute_fingerprints, retrain_feedback_model,\\\n",
    "                    create_drd2_dataset, combine_drd2_dataset, save_drd2_dataset\n",
    "                        \n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, matthews_corrcoef\n",
    "\n",
    "def predict_proba_from_model_fps(feedback_type, feedback_model, features):\n",
    "\n",
    "    # This is not computationally extensive yet\n",
    "    # choose float 32\n",
    "    \n",
    "    features = torch.tensor(features, dtype=torch.float32)\n",
    "    pred_label_proba = feedback_model.predict_proba(features).cpu().detach().numpy()\n",
    "    return pred_label_proba\n",
    "\n",
    "\n",
    "def check_create(path):\n",
    "    \"\"\"\n",
    "    Check if the directory exists, if not, create it.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "def evaluate_results(\n",
    "        output_dir, benchmark, # Choose whether to evaluate the results of the benchmark\n",
    "        feedback_type, # scoring, comparing, ranking\n",
    "        initial_feedback_model_path, # path to the initial feedback model\n",
    "        final_testing_dataset_path, # path to the final training dataset\n",
    "        model_pretrained_name, # Name of the pretrained model before REINVENT_round_1\n",
    "        num_rounds, # number of rounds, corresponding to R in the paper\n",
    "        num_iters, # number of iterations of showing molecules to the human for feedback at each round, corresponding to T in the paper\n",
    "):\n",
    "    \"\"\"\n",
    "    This function returns five dictionaries of benchmark results\n",
    "    \n",
    "    benchmark_1: ML model metrics (metrics dict)\n",
    "        - REINVENT_round_1\n",
    "          - accuracy\n",
    "          - precision\n",
    "          ...\n",
    "        - REINVENT_round_2\n",
    "        - ...\n",
    "    \n",
    "    benchmark_2: DRD2 probability distribution of REINVENT generated molecules (drd2_proba dict)\n",
    "        - REINVENT_round_1\n",
    "          - np.array of probabilities\n",
    "        - REINVENT_round_2\n",
    "        - ...\n",
    "\n",
    "    benchmark_3: Novelty of REINVENT generated molecules (novelty dict)\n",
    "        - REINVENT_round_1\n",
    "          - np.array of novelty scores\n",
    "        - REINVENT_round_2\n",
    "        - ...\n",
    "    \n",
    "    benchmark_4: Synthetic accessibility of REINVENT generated molecules (sa dict)\n",
    "        - REINVENT_round_1\n",
    "          - np.array of synthetic accessibility scores\n",
    "        - REINVENT_round_2\n",
    "        - ...\n",
    "\n",
    "    benchmark_5: logP of REINVENT generated molecules (logP dict)\n",
    "        - REINVENT_round_1\n",
    "          - np.array of logP scores\n",
    "        - REINVENT_round_2\n",
    "        - ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    final_testing_dataset = pd.read_csv(final_testing_dataset_path)\n",
    "\n",
    "    print(\"Loading final testing dataset successfully\")\n",
    "        \n",
    "    smiles_test = final_testing_dataset['smiles'].to_numpy()\n",
    "    label_test = final_testing_dataset['label'].to_numpy()\n",
    "    \n",
    "    # for score regression, dataset_outputs have columns: smiles, features, label_proba, label_binary\n",
    "    # for bradley-terry, dataset_outputs have columns: smiles_1, smiles_2, features_1, features_2, \n",
    "                                                      # label_1_proba, label_2_proba, label_1_binary, label_2_binary\n",
    "                                                      # compare_proba, compare_binary\n",
    "    # for rank listnet, dataset_outputs have columns: smiles_1/2/3, features_1/2/3, label_1/2/3_proba, label_1/2/3_binary\n",
    "                                                     # label_1/2/3_softmax, label_1/2/3_rank\n",
    "    \n",
    "    ###########################\n",
    "    # BENCHMARK 1: ML METRICS #\n",
    "    ###########################\n",
    "    \n",
    "    features_list = [compute_fingerprints(smiles) for smiles in smiles_test]\n",
    "    features = np.array(features_list, dtype=np.float32)\n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    if benchmark[\"metrics\"] == True:\n",
    "        metrics_model = {\n",
    "            'thresholds': [],\n",
    "            'TP': [], 'TN': [], 'FP': [], 'FN': [],\n",
    "            'accuracy': [], 'precision': [], 'recall': [], 'F1': [], 'MCC': [],\n",
    "        }\n",
    "        \n",
    "        initial_feedback_model = load_feedback_model(feedback_type=feedback_type, \n",
    "                                                    feedback_model_path=initial_feedback_model_path)\n",
    "        predicted_scores = predict_proba_from_model_fps(feedback_type, initial_feedback_model, features)\n",
    "\n",
    "        # Compute metrics over a range of thresholds\n",
    "        thresholds = np.linspace(0, 1, 101)\n",
    "        for threshold in thresholds:\n",
    "            predicted_labels = (predicted_scores > threshold).astype(int)\n",
    "            TP = np.sum((predicted_labels == 1) & (label_test == 1))\n",
    "            TN = np.sum((predicted_labels == 0) & (label_test == 0))\n",
    "            FP = np.sum((predicted_labels == 1) & (label_test == 0))\n",
    "            FN = np.sum((predicted_labels == 0) & (label_test == 1))\n",
    "\n",
    "            accuracy = (TP + TN) / len(label_test)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(label_test, predicted_labels, average='binary')\n",
    "            mcc = matthews_corrcoef(label_test, predicted_labels)\n",
    "\n",
    "            metrics_model['thresholds'].append(threshold)\n",
    "            metrics_model['TP'].append(TP)\n",
    "            metrics_model['TN'].append(TN)\n",
    "            metrics_model['FP'].append(FP)\n",
    "            metrics_model['FN'].append(FN)\n",
    "            metrics_model['accuracy'].append(accuracy)\n",
    "            metrics_model['precision'].append(precision)\n",
    "            metrics_model['recall'].append(recall)\n",
    "            metrics_model['F1'].append(f1)\n",
    "            metrics_model['MCC'].append(mcc)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(label_test, predicted_scores, pos_label=1)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        metrics_model['fpr'] = fpr\n",
    "        metrics_model['tpr'] = tpr\n",
    "        metrics_model['roc_auc'] = roc_auc\n",
    "        metrics[\"REINVENT_round_1\"] = metrics_model\n",
    "        \n",
    "        ########################\n",
    "        # HITL MODEL BENCHMARK #\n",
    "        ########################\n",
    "\n",
    "        for REINVENT_round in range(2, num_rounds + 2):\n",
    "            reinvent_round_name = f\"REINVENT_round_{REINVENT_round}\"\n",
    "            \n",
    "            hitl_iteration_name = f\"HITL_iteration_{num_iters}\"\n",
    "            HITL_iteration = num_iters\n",
    "            # for HITL_iteration in range(1, num_iters+1):\n",
    "            #     hitl_iteration_name = f\"HITL_iteration_{HITL_iteration}\"    \n",
    "\n",
    "            metrics_model = {\n",
    "                'thresholds': [],\n",
    "                'TP': [], 'TN': [], 'FP': [], 'FN': [],\n",
    "                'accuracy': [], 'precision': [], 'recall': [], 'F1': [], 'MCC': [],\n",
    "            }\n",
    "\n",
    "            feedback_model_path = f\"{output_dir}/REINVENT_round_{REINVENT_round - 1}/HITL_iteration_{HITL_iteration}/{model_pretrained_name}\"\n",
    "\n",
    "            if not os.path.exists(feedback_model_path):\n",
    "                print(f\"Feedback model {feedback_model_path} does not exist.\")\n",
    "            else:\n",
    "                feedback_model = load_feedback_model(feedback_type=feedback_type, feedback_model_path=feedback_model_path)\n",
    "                predicted_scores = predict_proba_from_model_fps(feedback_type, feedback_model, features)\n",
    "\n",
    "                # Compute metrics over a range of thresholds\n",
    "                thresholds = np.linspace(0, 1, 101)\n",
    "                for threshold in thresholds:\n",
    "                    predicted_labels = (predicted_scores > threshold).astype(int)\n",
    "                    TP = np.sum((predicted_labels == 1) & (label_test == 1))\n",
    "                    TN = np.sum((predicted_labels == 0) & (label_test == 0))\n",
    "                    FP = np.sum((predicted_labels == 1) & (label_test == 0))\n",
    "                    FN = np.sum((predicted_labels == 0) & (label_test == 1))\n",
    "                    \n",
    "                    accuracy = (TP + TN) / len(label_test)\n",
    "                    precision, recall, f1, _ = precision_recall_fscore_support(label_test, predicted_labels, average='binary')\n",
    "                    mcc = matthews_corrcoef(label_test, predicted_labels)\n",
    "                    \n",
    "                    metrics_model['thresholds'].append(threshold)\n",
    "                    metrics_model['TP'].append(TP)\n",
    "                    metrics_model['TN'].append(TN)\n",
    "                    metrics_model['FP'].append(FP)\n",
    "                    metrics_model['FN'].append(FN)\n",
    "                    metrics_model['accuracy'].append(accuracy)\n",
    "                    metrics_model['precision'].append(precision)\n",
    "                    metrics_model['recall'].append(recall)\n",
    "                    metrics_model['F1'].append(f1)\n",
    "                    metrics_model['MCC'].append(mcc)\n",
    "        \n",
    "                # Compute ROC curve and AUC\n",
    "                fpr, tpr, _ = roc_curve(label_test, predicted_scores, pos_label=1)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "\n",
    "                metrics_model['fpr'] = fpr\n",
    "                metrics_model['tpr'] = tpr\n",
    "                metrics_model['roc_auc'] = roc_auc\n",
    "\n",
    "                #metrics[f\"{reinvent_round_name}_{hitl_iteration_name}\"] = metrics_model\n",
    "                metrics[reinvent_round_name] = metrics_model\n",
    "        \n",
    "    ##############################################\n",
    "    # BENCHMARK 2: DRD2 PROBABILITY DISTRIBUTION #\n",
    "    ##############################################\n",
    "\n",
    "    drd2_proba = {}\n",
    "\n",
    "    if benchmark[\"drd2_proba\"] == True:\n",
    "        \n",
    "        for REINVENT_round in range(1, num_rounds + 2):\n",
    "            reinvent_round_name = f\"REINVENT_round_{REINVENT_round}\"\n",
    "            \n",
    "            scaffold_memory_path = f\"{output_dir}/{reinvent_round_name}/results/scaffold_memory.csv\"\n",
    "            \n",
    "            if os.path.exists(scaffold_memory_path):\n",
    "                scaffold_df = pd.read_csv(scaffold_memory_path)\n",
    "                smiles_list_round = scaffold_df['SMILES']\n",
    "                \n",
    "                # Load the TDC Oracle\n",
    "                oracle = Oracle(name='DRD2')\n",
    "                \n",
    "                # Compute DRD2 probabilities\n",
    "                drd2_probs = [oracle(smiles) for smiles in smiles_list_round]\n",
    "                \n",
    "                # Store the results\n",
    "                drd2_proba[reinvent_round_name] = np.array(drd2_probs)\n",
    "            else:\n",
    "                print(f\"Scaffold memory file {scaffold_memory_path} does not exist.\")\n",
    "\n",
    "    ##############################\n",
    "    # BENCHMARK 3: NOVELTY SCORE #\n",
    "    ##############################\n",
    "\n",
    "    novelty_score = {}\n",
    "\n",
    "    if benchmark[\"novelty_score\"] == True:\n",
    "        \n",
    "        for REINVENT_round in range(1, num_rounds + 2):\n",
    "            reinvent_round_name = f\"REINVENT_round_{REINVENT_round}\"\n",
    "            scaffold_memory_path = f\"{output_dir}/{reinvent_round_name}/results/scaffold_memory.csv\"\n",
    "            \n",
    "            if os.path.exists(scaffold_memory_path):\n",
    "                scaffold_df = pd.read_csv(scaffold_memory_path)\n",
    "                smiles_list = scaffold_df['SMILES']\n",
    "                \n",
    "                novelty_scores = []\n",
    "\n",
    "                for smiles in smiles_list:\n",
    "                    mol = Chem.MolFromSmiles(smiles)\n",
    "                    if mol:\n",
    "                        fps = compute_fingerprints(smiles)\n",
    "                        # Reference fps is the testing features\n",
    "                        similarities = DataStructs.BulkTanimotoSimilarity(fps, features_list)\n",
    "                        max_similarity = max(similarities)\n",
    "                        novelty = 1 - max_similarity\n",
    "                        novelty_scores.append(novelty)\n",
    "                    else:\n",
    "                        novelty_scores.append(None)  # Handle invalid SMILES\n",
    "                    \n",
    "                # Store the results\n",
    "                novelty_score[reinvent_round_name] = np.array(novelty_scores)\n",
    "            else:\n",
    "                print(f\"Scaffold memory file {scaffold_memory_path} does not exist.\")\n",
    "\n",
    "\n",
    "    ##############################################\n",
    "    # BENCHMARK 4: SYNTHETIC ACCESSIBILITY SCORE #\n",
    "    ##############################################\n",
    "\n",
    "    sa_score = {}\n",
    "    \n",
    "    if benchmark[\"sa_score\"] == True:\n",
    "\n",
    "        for REINVENT_round in range(1, num_rounds + 2):\n",
    "            reinvent_round_name = f\"REINVENT_round_{REINVENT_round}\"\n",
    "            scaffold_memory_path = f\"{output_dir}/{reinvent_round_name}/results/scaffold_memory.csv\"\n",
    "            \n",
    "            if os.path.exists(scaffold_memory_path):\n",
    "                scaffold_df = pd.read_csv(scaffold_memory_path)\n",
    "                smiles_list = scaffold_df['SMILES']\n",
    "\n",
    "                sa_scores = []\n",
    "                for smiles in smiles_list:\n",
    "                    mol = Chem.MolFromSmiles(smiles)\n",
    "                    if mol:\n",
    "                        sa = sascorer.calculateScore(mol)\n",
    "                        sa_scores.append(sa)\n",
    "                    else:\n",
    "                        sa_scores.append(None)  # Handle invalid SMILES\n",
    "\n",
    "                # Store the results\n",
    "                sa_score[reinvent_round_name] = np.array(sa_scores)\n",
    "            else:\n",
    "                print(f\"Scaffold memory file {scaffold_memory_path} does not exist.\")\n",
    "                      \n",
    "\n",
    "    ###########################\n",
    "    # BENCHMARK 5: LOGP SCORE #\n",
    "    ###########################\n",
    "\n",
    "    logP_score = {}\n",
    "\n",
    "    if benchmark[\"logP_score\"] == True:\n",
    "        for REINVENT_round in range(1, num_rounds + 2):\n",
    "            reinvent_round_name = f\"REINVENT_round_{REINVENT_round}\"\n",
    "            scaffold_memory_path = f\"{output_dir}/{reinvent_round_name}/results/scaffold_memory.csv\"\n",
    "            \n",
    "            if os.path.exists(scaffold_memory_path):\n",
    "                scaffold_df = pd.read_csv(scaffold_memory_path)\n",
    "                smiles_list = scaffold_df['SMILES']\n",
    "\n",
    "                logP_scores = []\n",
    "                for smiles in smiles_list:\n",
    "                    mol = Chem.MolFromSmiles(smiles)\n",
    "                    if mol:\n",
    "                        logP = Crippen.MolLogP(mol)\n",
    "                        logP_scores.append(logP)\n",
    "                    else:\n",
    "                        logP_scores.append(None)  # Handle invalid SMILES\n",
    "\n",
    "                # Store the results\n",
    "                logP_score[reinvent_round_name] = np.array(logP_scores)\n",
    "            else:\n",
    "                print(f\"Scaffold memory file {scaffold_memory_path} does not exist.\")\n",
    "\n",
    "    return metrics, drd2_proba, novelty_score, sa_score, logP_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running score regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading final testing dataset successfully\n",
      "Loading Score Regression model from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Score_Regression_model/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_random_noise_0.0/REINVENT_round_1/HITL_iteration_5/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_random_noise_0.0/REINVENT_round_2/HITL_iteration_5/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_random_noise_0.0/REINVENT_round_3/HITL_iteration_5/score_regression_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Score Regression model from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Score_Regression_model/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_random_noise_0.1/REINVENT_round_1/HITL_iteration_5/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_random_noise_0.1/REINVENT_round_2/HITL_iteration_5/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_random_noise_0.1/REINVENT_round_3/HITL_iteration_5/score_regression_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Score Regression model from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Score_Regression_model/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_uncertainty_noise_0.0/REINVENT_round_1/HITL_iteration_5/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_uncertainty_noise_0.0/REINVENT_round_2/HITL_iteration_5/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_uncertainty_noise_0.0/REINVENT_round_3/HITL_iteration_5/score_regression_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Score Regression model from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Score_Regression_model/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_uncertainty_noise_0.1/REINVENT_round_1/HITL_iteration_5/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_uncertainty_noise_0.1/REINVENT_round_2/HITL_iteration_5/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_uncertainty_noise_0.1/REINVENT_round_3/HITL_iteration_5/score_regression_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Score Regression model from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Score_Regression_model/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_greedy_noise_0.0/REINVENT_round_1/HITL_iteration_5/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_greedy_noise_0.0/REINVENT_round_2/HITL_iteration_5/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_greedy_noise_0.0/REINVENT_round_3/HITL_iteration_5/score_regression_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Score Regression model from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Score_Regression_model/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_greedy_noise_0.1/REINVENT_round_1/HITL_iteration_5/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_greedy_noise_0.1/REINVENT_round_2/HITL_iteration_5/score_regression_model.pth\n",
      "Loading Score Regression model from output_score_regression/R3_T5_Q20_acq_greedy_noise_0.1/REINVENT_round_3/HITL_iteration_5/score_regression_model.pth\n"
     ]
    }
   ],
   "source": [
    "feedback_type = \"scoring\" \n",
    "\n",
    "# feedback type as scoring:\n",
    "# Given a molecule, what is the probability that the molecule is active regarding DRD2?  \n",
    "\n",
    "num_rounds = 3 # number of rounds, corresponding to R in the paper\n",
    "num_iters = 5 # number of iterations of showing molecules to the human for feedback at each round\n",
    "num_queries = 20 # number of molecules, pairs or a set of molecules, dependig on the task, \n",
    "                 # shown to the simulated chemist at each HITL_iteration\n",
    "\n",
    "for acquisition in [\"random\", \"uncertainty\", \"greedy\"]:\n",
    "    for sigma_noise in [0.0, 0.1]:\n",
    "        output_dir = f\"output_score_regression/R{num_rounds}_T{num_iters}_Q{num_queries}_acq_{acquisition}_noise_{sigma_noise}\"\n",
    "        initial_feedback_model_path = f\"/home/springnuance/reinvent-hitl/Base-Code-Binh/training_Score_Regression_model/score_regression_model.pth\"\n",
    "        final_testing_dataset_path = \"/home/springnuance/reinvent-hitl/Base-Code-Binh/data/drd2_final_test_large.csv\"\n",
    "\n",
    "        model_pretrained_name = \"score_regression_model.pth\"\n",
    "\n",
    "        benchmark = {\n",
    "            \"metrics\": True,\n",
    "            \"drd2_proba\": False,\n",
    "            \"novelty_score\": False,\n",
    "            \"sa_score\": False,\n",
    "            \"logP_score\": False,\n",
    "        }\n",
    "\n",
    "        metrics, drd2_proba, novelty_score, sa_score, logP_score = evaluate_results(\n",
    "                output_dir, benchmark,\n",
    "                feedback_type, # scoring, comparing, ranking\n",
    "                initial_feedback_model_path,\n",
    "                final_testing_dataset_path, # path to the final training dataset\n",
    "                model_pretrained_name, # Name of the pretrained model before REINVENT_round_1\n",
    "                num_rounds, # number of rounds, corresponding to R in the paper\n",
    "                num_iters, # number of iterations of showing molecules to the human for feedback at each round, corresponding to T in the paper\n",
    "        )\n",
    "\n",
    "        if benchmark[\"metrics\"] == True:\n",
    "            np.save(f\"results_score_regression/acq_{acquisition}_noise_{sigma_noise}_metrics.npy\", metrics)\n",
    "        if benchmark[\"drd2_proba\"] == True:\n",
    "            np.save(f\"results_score_regression/acq_{acquisition}_noise_{sigma_noise}_drd2_proba.npy\", drd2_proba)\n",
    "        if benchmark[\"novelty_score\"] == True:\n",
    "            np.save(f\"results_score_regression/acq_{acquisition}_noise_{sigma_noise}_novelty_score.npy\", novelty_score)\n",
    "        if benchmark[\"sa_score\"] == True:\n",
    "            np.save(f\"results_score_regression/acq_{acquisition}_noise_{sigma_noise}_sa_score.npy\", sa_score)\n",
    "        if benchmark[\"logP_score\"] == True:\n",
    "            np.save(f\"results_score_regression/acq_{acquisition}_noise_{sigma_noise}_logP_score.npy\", logP_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Bradley Terry model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading final testing dataset successfully\n",
      "Loading Bradley Terry model successfully from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Bradley_Terry_model/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_random_noise_0.0/REINVENT_round_1/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_random_noise_0.0/REINVENT_round_2/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_random_noise_0.0/REINVENT_round_3/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Bradley Terry model successfully from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Bradley_Terry_model/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_random_noise_0.1/REINVENT_round_1/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_random_noise_0.1/REINVENT_round_2/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_random_noise_0.1/REINVENT_round_3/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Bradley Terry model successfully from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Bradley_Terry_model/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_uncertainty_noise_0.0/REINVENT_round_1/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_uncertainty_noise_0.0/REINVENT_round_2/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_uncertainty_noise_0.0/REINVENT_round_3/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Bradley Terry model successfully from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Bradley_Terry_model/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_uncertainty_noise_0.1/REINVENT_round_1/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_uncertainty_noise_0.1/REINVENT_round_2/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_uncertainty_noise_0.1/REINVENT_round_3/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Bradley Terry model successfully from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Bradley_Terry_model/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_greedy_noise_0.0/REINVENT_round_1/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_greedy_noise_0.0/REINVENT_round_2/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_greedy_noise_0.0/REINVENT_round_3/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Bradley Terry model successfully from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Bradley_Terry_model/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_greedy_noise_0.1/REINVENT_round_1/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_greedy_noise_0.1/REINVENT_round_2/HITL_iteration_5/bradley_terry_model.pth\n",
      "Loading Bradley Terry model successfully from output_bradley_terry/R3_T5_Q5_acq_greedy_noise_0.1/REINVENT_round_3/HITL_iteration_5/bradley_terry_model.pth\n"
     ]
    }
   ],
   "source": [
    "feedback_type = \"comparing\" # scoring, comparing, ranking\n",
    "\n",
    "# feedback type as comparing:\n",
    "# Given two molecules, what is the probability that the first molecule is more active than the second molecule regarding DRD2?\n",
    "\n",
    "num_rounds = 3 # number of rounds, corresponding to R in the paper\n",
    "num_iters = 5 # number of iterations of showing molecules to the human for feedback at each round\n",
    "num_queries = 5 # number of molecules, pairs or a set of molecules, dependig on the task, \n",
    "                 # shown to the simulated chemist at each HITL_iteration\n",
    "\n",
    "for acquisition in [\"random\", \"uncertainty\", \"greedy\"]:\n",
    "    for sigma_noise in [0.0, 0.1]:\n",
    "        output_dir = f\"output_bradley_terry/R{num_rounds}_T{num_iters}_Q{num_queries}_acq_{acquisition}_noise_{sigma_noise}\"\n",
    "        initial_feedback_model_path = f\"/home/springnuance/reinvent-hitl/Base-Code-Binh/training_Bradley_Terry_model/bradley_terry_model.pth\"\n",
    "        final_testing_dataset_path = \"/home/springnuance/reinvent-hitl/Base-Code-Binh/data/drd2_final_test_large.csv\"\n",
    "\n",
    "        model_pretrained_name = \"bradley_terry_model.pth\"\n",
    "\n",
    "        benchmark = {\n",
    "            \"metrics\": True,\n",
    "            \"drd2_proba\": False,\n",
    "            \"novelty_score\": False,\n",
    "            \"sa_score\": False,\n",
    "            \"logP_score\": False,\n",
    "        }\n",
    "\n",
    "        metrics, drd2_proba, novelty_score, sa_score, logP_score = evaluate_results(\n",
    "                output_dir, benchmark,\n",
    "                feedback_type, # scoring, comparing, ranking\n",
    "                initial_feedback_model_path,\n",
    "                final_testing_dataset_path, # path to the final training dataset\n",
    "                model_pretrained_name, # Name of the pretrained model before REINVENT_round_1\n",
    "                num_rounds, # number of rounds, corresponding to R in the paper\n",
    "                num_iters, # number of iterations of showing molecules to the human for feedback at each round, corresponding to T in the paper\n",
    "        )\n",
    "\n",
    "        if benchmark[\"metrics\"] == True:\n",
    "            np.save(f\"results_bradley_terry/acq_{acquisition}_noise_{sigma_noise}_metrics.npy\", metrics)\n",
    "        if benchmark[\"drd2_proba\"] == True:\n",
    "            np.save(f\"results_bradley_terry/acq_{acquisition}_noise_{sigma_noise}_drd2_proba.npy\", drd2_proba)\n",
    "        if benchmark[\"novelty_score\"] == True:\n",
    "            np.save(f\"results_bradley_terry/acq_{acquisition}_noise_{sigma_noise}_novelty_score.npy\", novelty_score)\n",
    "        if benchmark[\"sa_score\"] == True:\n",
    "            np.save(f\"results_bradley_terry/acq_{acquisition}_noise_{sigma_noise}_sa_score.npy\", sa_score)\n",
    "        if benchmark[\"logP_score\"] == True:\n",
    "            np.save(f\"results_bradley_terry/acq_{acquisition}_noise_{sigma_noise}_logP_score.npy\", logP_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Rank ListNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading final testing dataset successfully\n",
      "Loading Rank ListNet model successfully from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Rank_ListNet_model/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_random_noise_0.0/REINVENT_round_1/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_random_noise_0.0/REINVENT_round_2/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_random_noise_0.0/REINVENT_round_3/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Rank ListNet model successfully from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Rank_ListNet_model/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_random_noise_0.1/REINVENT_round_1/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_random_noise_0.1/REINVENT_round_2/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_random_noise_0.1/REINVENT_round_3/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Rank ListNet model successfully from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Rank_ListNet_model/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_uncertainty_noise_0.0/REINVENT_round_1/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_uncertainty_noise_0.0/REINVENT_round_2/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_uncertainty_noise_0.0/REINVENT_round_3/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Rank ListNet model successfully from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Rank_ListNet_model/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_uncertainty_noise_0.1/REINVENT_round_1/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_uncertainty_noise_0.1/REINVENT_round_2/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_uncertainty_noise_0.1/REINVENT_round_3/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Rank ListNet model successfully from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Rank_ListNet_model/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_greedy_noise_0.0/REINVENT_round_1/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_greedy_noise_0.0/REINVENT_round_2/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_greedy_noise_0.0/REINVENT_round_3/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading final testing dataset successfully\n",
      "Loading Rank ListNet model successfully from /home/springnuance/reinvent-hitl/Base-Code-Binh/training_Rank_ListNet_model/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_greedy_noise_0.1/REINVENT_round_1/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_greedy_noise_0.1/REINVENT_round_2/HITL_iteration_5/rank_listnet_model.pth\n",
      "Loading Rank ListNet model successfully from output_rank_listnet/R3_T5_Q6_acq_greedy_noise_0.1/REINVENT_round_3/HITL_iteration_5/rank_listnet_model.pth\n"
     ]
    }
   ],
   "source": [
    "feedback_type = \"ranking\" # scoring, comparing, ranking\n",
    "\n",
    "# feedback type as ranking:\n",
    "# Given N molecules, what are the orders of preference of these molecules regarding DRD2?\n",
    "\n",
    "num_rounds = 3 # number of rounds, corresponding to R in the paper\n",
    "num_iters = 5 # number of iterations of showing molecules to the human for feedback at each round\n",
    "num_queries = 6 # number of molecules, pairs or a set of molecules, depending on the task, \n",
    "                 # shown to the simulated chemist at each HITL_iteration\n",
    "\n",
    "for acquisition in [\"random\", \"uncertainty\", \"greedy\"]:\n",
    "    for sigma_noise in [0.0, 0.1]:\n",
    "        output_dir = f\"output_rank_listnet/R{num_rounds}_T{num_iters}_Q{num_queries}_acq_{acquisition}_noise_{sigma_noise}\"\n",
    "        initial_feedback_model_path = f\"/home/springnuance/reinvent-hitl/Base-Code-Binh/training_Rank_ListNet_model/rank_listnet_model.pth\"\n",
    "        final_testing_dataset_path = \"/home/springnuance/reinvent-hitl/Base-Code-Binh/data/drd2_final_test_large.csv\"\n",
    "\n",
    "        model_pretrained_name = \"rank_listnet_model.pth\"\n",
    "\n",
    "        benchmark = {\n",
    "            \"metrics\": True,\n",
    "            \"drd2_proba\": False,\n",
    "            \"novelty_score\": False,\n",
    "            \"sa_score\": False,\n",
    "            \"logP_score\": False,\n",
    "        }\n",
    "\n",
    "        metrics, drd2_proba, novelty_score, sa_score, logP_score = evaluate_results(\n",
    "                output_dir, benchmark,\n",
    "                feedback_type, # scoring, comparing, ranking\n",
    "                initial_feedback_model_path,\n",
    "                final_testing_dataset_path, # path to the final training dataset\n",
    "                model_pretrained_name, # Name of the pretrained model before REINVENT_round_1\n",
    "                num_rounds, # number of rounds, corresponding to R in the paper\n",
    "                num_iters, # number of iterations of showing molecules to the human for feedback at each round, corresponding to T in the paper\n",
    "        )\n",
    "\n",
    "        if benchmark[\"metrics\"] == True:\n",
    "            np.save(f\"results_rank_listnet/acq_{acquisition}_noise_{sigma_noise}_metrics.npy\", metrics)\n",
    "        if benchmark[\"drd2_proba\"] == True:\n",
    "            np.save(f\"results_rank_listnet/acq_{acquisition}_noise_{sigma_noise}_drd2_proba.npy\", drd2_proba)\n",
    "        if benchmark[\"novelty_score\"] == True:\n",
    "            np.save(f\"results_rank_listnet/acq_{acquisition}_noise_{sigma_noise}_novelty_score.npy\", novelty_score)\n",
    "        if benchmark[\"sa_score\"] == True:\n",
    "            np.save(f\"results_rank_listnet/acq_{acquisition}_noise_{sigma_noise}_sa_score.npy\", sa_score)\n",
    "        if benchmark[\"logP_score\"] == True:\n",
    "            np.save(f\"results_rank_listnet/acq_{acquisition}_noise_{sigma_noise}_logP_score.npy\", logP_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc_env_hitl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
